Warning : ASCEND_HOME_PATH environment variable is not set.
/usr/local/Ascend/ascend-toolkit/latest/tools/ms_fmk_transplt/torch_npu_bridge/transfer_to_npu.py:160: ImportWarning: 
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.autocast, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************
    
  warnings.warn(msg, ImportWarning)
CUDA: True cuda:0
Read configuration file: configurations/PREDICT.conf
!!!!num_of_hours 1
!!!!num_of_days 0
!!!!num_of_hours 1
!!!!num_of_weeks 0
!!!!DEVICE cuda:0
!!!!batch_size 12
folder_dir: MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TE
TemporalPositionalEncoding max_len: 12
w_index: []
d_index: []
h_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
en_lookup_index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]

 EncoderDecoder(
  (encoder): Encoder(
    (layers): ModuleList(
      (0): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_q1d_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Decoder(
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (3): DecoderLayer(
        (self_attn): MultiHeadAttentionAwareTemporalContex_qc_kc(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (conv1Ds_aware_temporal_context): ModuleList(
            (0): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
            (1): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (src_attn): MultiHeadAttentionAwareTemporalContex_qc_k1d(
          (linears): ModuleList(
            (0): Linear(in_features=64, out_features=64, bias=True)
            (1): Linear(in_features=64, out_features=64, bias=True)
          )
          (query_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))
          (key_conv1Ds_aware_temporal_context): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward_gcn): PositionWiseGCNFeedForward(
          (gcn): spatialAttentionScaledGCN(
            (Theta): Linear(in_features=64, out_features=64, bias=False)
            (SAt): Spatial_Attention_layer(
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (1): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
          (2): SublayerConnection(
            (dropout): Dropout(p=0.0, inplace=False)
            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (src_embed): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(15, 64)
    )
  )
  (trg_embed): Sequential(
    (0): Linear(in_features=1, out_features=64, bias=True)
    (1): TemporalPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (2): SpatialPositionalEncoding(
      (dropout): Dropout(p=0.0, inplace=False)
      (embedding): Embedding(15, 64)
    )
  )
  (prediction_generator): Linear(in_features=64, out_features=1, bias=True)
)
loading model done!
 * Serving Flask app 'inference_api'
 * Debug mode: off
2023-11-09 13:24:41 [INFO] [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:1125
 * Running on http://192.168.1.3:1125
2023-11-09 13:24:41 [INFO] [33mPress CTRL+C to quit[0m
2023-11-09 13:27:13 [INFO] 218.247.253.163 - - [09/Nov/2023 13:27:13] "[33mPOST / HTTP/1.1[0m" 404 -
2023-11-09 13:27:26 [ERROR] 89.248.163.96 - - [09/Nov/2023 13:27:26] code 400, message Bad HTTP/0.9 request type ('\x03\x00\x00/*à\x00\x00\x00\x00\x00Cookie:')
2023-11-09 13:27:26 [INFO] 89.248.163.96 - - [09/Nov/2023 13:27:26] "[35m[1m  /*à     Cookie: mstshash=Administr[0m" HTTPStatus.BAD_REQUEST -
2023-11-09 13:31:20 [INFO] 218.247.253.163 - - [09/Nov/2023 13:31:20] "[33mPOST / HTTP/1.1[0m" 404 -
2023-11-09 13:31:22 [INFO] 218.247.253.163 - - [09/Nov/2023 13:31:22] "[33mPOST / HTTP/1.1[0m" 404 -
2023-11-09 13:31:24 [INFO] 218.247.253.163 - - [09/Nov/2023 13:31:24] "[33mPOST / HTTP/1.1[0m" 404 -
2023-11-09 13:34:48 [INFO] 218.247.253.163 - - [09/Nov/2023 13:34:48] "[33mPOST / HTTP/1.1[0m" 404 -
2023-11-09 13:34:49 [INFO] 218.247.253.163 - - [09/Nov/2023 13:34:49] "[33mPOST / HTTP/1.1[0m" 404 -
2023-11-09 13:34:50 [INFO] 218.247.253.163 - - [09/Nov/2023 13:34:50] "[33mPOST / HTTP/1.1[0m" 404 -
2023-11-09 13:34:54 [INFO] 218.247.253.163 - - [09/Nov/2023 13:34:54] "[33mPOST / HTTP/1.1[0m" 404 -
2023-11-09 13:34:55 [INFO] 218.247.253.163 - - [09/Nov/2023 13:34:55] "[33mPOST / HTTP/1.1[0m" 404 -
2023-11-09 13:35:06 [INFO] 218.247.253.163 - - [09/Nov/2023 13:35:06] "[33mPOST / HTTP/1.1[0m" 404 -
2023-11-09 13:35:28 [INFO] 218.247.253.163 - - [09/Nov/2023 13:35:28] "[33mPOST / HTTP/1.1[0m" 404 -
2023-11-09 13:38:31 [INFO] 218.247.253.163 - - [09/Nov/2023 13:38:31] "POST /TrafficFlowForcast HTTP/1.1" 200 -
2023-11-09 13:38:32 [INFO] 218.247.253.163 - - [09/Nov/2023 13:38:32] "POST /TrafficFlowForcast HTTP/1.1" 200 -
2023-11-09 13:38:33 [INFO] 218.247.253.163 - - [09/Nov/2023 13:38:33] "POST /TrafficFlowForcast HTTP/1.1" 200 -
2023-11-09 13:39:34 [INFO] 218.247.253.163 - - [09/Nov/2023 13:39:34] "POST /TrafficFlowForcast HTTP/1.1" 200 -
2023-11-09 13:39:43 [INFO] 218.247.253.163 - - [09/Nov/2023 13:39:43] "POST /TrafficFlowForcast HTTP/1.1" 200 -
2023-11-09 13:39:44 [INFO] 218.247.253.163 - - [09/Nov/2023 13:39:44] "POST /TrafficFlowForcast HTTP/1.1" 200 -
!!!!!!!!1
CUDA: True cuda:0
Read configuration file: configurations/PREDICT.conf
graph_signal_matrix_filenamedata/PREDICT/20231101.npy
!!!!num_of_hours 1
!!!!num_of_days 0
!!!!num_of_hours 1
!!!!num_of_weeks 0
!!!!DEVICE cuda:0
!!!!batch_size 12
file: 20231101
load file: data/PREDICT/20231101_r1_d0_w0.npz
ori length: 288 , percent: 1.0 , scale: 288
text_x shape: (288, 15, 3, 12)
after cut text_x shape: (12, 15, 3, 12)
train: torch.Size([288, 15, 1, 12]) torch.Size([288, 15, 12]) torch.Size([288, 15, 12])
val: torch.Size([288, 15, 1, 12]) torch.Size([288, 15, 12]) torch.Size([288, 15, 12])
test: torch.Size([12, 15, 1, 12]) torch.Size([12, 15, 12]) torch.Size([12, 15, 12])
!!!!!!!2
!!!!!!!!3

load weight from: ./experiments/PREDICT/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TE/epoch_100.params
-/root/ASTGCN2/code/ASTGNN_ascend/ASTGNN_new/model/ASTGNN.py:203: UserWarning: AutoNonVariableTypeMode is deprecated and will be removed in 1.10 release. For kernel implementations please use AutoDispatchBelowADInplaceOrView instead, If you are looking for a user facing API to enable running your inference-only workload, please use c10::InferenceMode. Using AutoDispatchBelowADInplaceOrView in user code is under risk of producing silent wrong result in some edge cases. See Note [AutoDispatchBelowAutograd] for more details. (Triggered internally at  /opt/buildtools/python-3.7.5/lib/python3.7/site-packages/torch/include/ATen/core/LegacyTypeDispatch.h:79.)
  x = x + self.pe[:, :, self.lookup_index, :]  # (batch_size, N, T, F_in) + (1,1,T,d_model)


#########
tensor([[[-0.8689]],

        [[-0.7842]],

        [[-0.8954]],

        [[-0.7569]],

        [[-0.5042]],

        [[-0.9252]],

        [[-0.9983]],

        [[-0.6721]],

        [[-0.8140]],

        [[-0.8182]],

        [[-0.8601]],

        [[-0.8636]],

        [[-0.6343]],

        [[-0.9650]],

        [[-0.4969]]], device='npu:0')
########




#########
tensor([[[-0.8689],
         [-0.8615]],

        [[-0.7842],
         [-0.7860]],

        [[-0.8954],
         [-0.8936]],

        [[-0.7569],
         [-0.7481]],

        [[-0.5042],
         [-0.5442]],

        [[-0.9252],
         [-0.9254]],

        [[-0.9983],
         [-0.9979]],

        [[-0.6721],
         [-0.6719]],

        [[-0.8140],
         [-0.8193]],

        [[-0.8182],
         [-0.8115]],

        [[-0.8601],
         [-0.8553]],

        [[-0.8636],
         [-0.8579]],

        [[-0.6343],
         [-0.6238]],

        [[-0.9650],
         [-0.9678]],

        [[-0.4969],
         [-0.5010]]], device='npu:0')
########




#########
tensor([[[-0.8689],
         [-0.8615],
         [-0.8699]],

        [[-0.7842],
         [-0.7860],
         [-0.8016]],

        [[-0.8954],
         [-0.8936],
         [-0.8998]],

        [[-0.7569],
         [-0.7481],
         [-0.7510]],

        [[-0.5042],
         [-0.5442],
         [-0.5786]],

        [[-0.9252],
         [-0.9254],
         [-0.9315]],

        [[-0.9983],
         [-0.9979],
         [-0.9980]],

        [[-0.6721],
         [-0.6719],
         [-0.6816]],

        [[-0.8140],
         [-0.8193],
         [-0.8260]],

        [[-0.8182],
         [-0.8115],
         [-0.8181]],

        [[-0.8601],
         [-0.8553],
         [-0.8529]],

        [[-0.8636],
         [-0.8579],
         [-0.8641]],

        [[-0.6343],
         [-0.6238],
         [-0.6411]],

        [[-0.9650],
         [-0.9678],
         [-0.9694]],

        [[-0.4969],
         [-0.5010],
         [-0.4945]]], device='npu:0')
########




#########
tensor([[[-0.8689],
         [-0.8615],
         [-0.8699],
         [-0.8736]],

        [[-0.7842],
         [-0.7860],
         [-0.8016],
         [-0.8044]],

        [[-0.8954],
         [-0.8936],
         [-0.8998],
         [-0.9022]],

        [[-0.7569],
         [-0.7481],
         [-0.7510],
         [-0.7532]],

        [[-0.5042],
         [-0.5442],
         [-0.5786],
         [-0.5705]],

        [[-0.9252],
         [-0.9254],
         [-0.9315],
         [-0.9333]],

        [[-0.9983],
         [-0.9979],
         [-0.9980],
         [-0.9980]],

        [[-0.6721],
         [-0.6719],
         [-0.6816],
         [-0.6910]],

        [[-0.8140],
         [-0.8193],
         [-0.8260],
         [-0.8285]],

        [[-0.8182],
         [-0.8115],
         [-0.8181],
         [-0.8204]],

        [[-0.8601],
         [-0.8553],
         [-0.8529],
         [-0.8584]],

        [[-0.8636],
         [-0.8579],
         [-0.8641],
         [-0.8686]],

        [[-0.6343],
         [-0.6238],
         [-0.6411],
         [-0.6486]],

        [[-0.9650],
         [-0.9678],
         [-0.9694],
         [-0.9708]],

        [[-0.4969],
         [-0.5010],
         [-0.4945],
         [-0.5093]]], device='npu:0')
########




#########
tensor([[[-0.8689],
         [-0.8615],
         [-0.8699],
         [-0.8736],
         [-0.8788]],

        [[-0.7842],
         [-0.7860],
         [-0.8016],
         [-0.8044],
         [-0.8084]],

        [[-0.8954],
         [-0.8936],
         [-0.8998],
         [-0.9022],
         [-0.9035]],

        [[-0.7569],
         [-0.7481],
         [-0.7510],
         [-0.7532],
         [-0.7577]],

        [[-0.5042],
         [-0.5442],
         [-0.5786],
         [-0.5705],
         [-0.5791]],

        [[-0.9252],
         [-0.9254],
         [-0.9315],
         [-0.9333],
         [-0.9352]],

        [[-0.9983],
         [-0.9979],
         [-0.9980],
         [-0.9980],
         [-0.9977]],

        [[-0.6721],
         [-0.6719],
         [-0.6816],
         [-0.6910],
         [-0.7077]],

        [[-0.8140],
         [-0.8193],
         [-0.8260],
         [-0.8285],
         [-0.8402]],

        [[-0.8182],
         [-0.8115],
         [-0.8181],
         [-0.8204],
         [-0.8286]],

        [[-0.8601],
         [-0.8553],
         [-0.8529],
         [-0.8584],
         [-0.8652]],

        [[-0.8636],
         [-0.8579],
         [-0.8641],
         [-0.8686],
         [-0.8730]],

        [[-0.6343],
         [-0.6238],
         [-0.6411],
         [-0.6486],
         [-0.6655]],

        [[-0.9650],
         [-0.9678],
         [-0.9694],
         [-0.9708],
         [-0.9727]],

        [[-0.4969],
         [-0.5010],
         [-0.4945],
         [-0.5093],
         [-0.5244]]], device='npu:0')
########




#########
tensor([[[-0.8689],
         [-0.8615],
         [-0.8699],
         [-0.8736],
         [-0.8788],
         [-0.8828]],

        [[-0.7842],
         [-0.7860],
         [-0.8016],
         [-0.8044],
         [-0.8084],
         [-0.8136]],

        [[-0.8954],
         [-0.8936],
         [-0.8998],
         [-0.9022],
         [-0.9035],
         [-0.9077]],

        [[-0.7569],
         [-0.7481],
         [-0.7510],
         [-0.7532],
         [-0.7577],
         [-0.7681]],

        [[-0.5042],
         [-0.5442],
         [-0.5786],
         [-0.5705],
         [-0.5791],
         [-0.5887]],

        [[-0.9252],
         [-0.9254],
         [-0.9315],
         [-0.9333],
         [-0.9352],
         [-0.9381]],

        [[-0.9983],
         [-0.9979],
         [-0.9980],
         [-0.9980],
         [-0.9977],
         [-0.9978]],

        [[-0.6721],
         [-0.6719],
         [-0.6816],
         [-0.6910],
         [-0.7077],
         [-0.7195]],

        [[-0.8140],
         [-0.8193],
         [-0.8260],
         [-0.8285],
         [-0.8402],
         [-0.8469]],

        [[-0.8182],
         [-0.8115],
         [-0.8181],
         [-0.8204],
         [-0.8286],
         [-0.8375]],

        [[-0.8601],
         [-0.8553],
         [-0.8529],
         [-0.8584],
         [-0.8652],
         [-0.8710]],

        [[-0.8636],
         [-0.8579],
         [-0.8641],
         [-0.8686],
         [-0.8730],
         [-0.8780]],

        [[-0.6343],
         [-0.6238],
         [-0.6411],
         [-0.6486],
         [-0.6655],
         [-0.6838]],

        [[-0.9650],
         [-0.9678],
         [-0.9694],
         [-0.9708],
         [-0.9727],
         [-0.9742]],

        [[-0.4969],
         [-0.5010],
         [-0.4945],
         [-0.5093],
         [-0.5244],
         [-0.5484]]], device='npu:0')
########




#########
tensor([[[-0.8689],
         [-0.8615],
         [-0.8699],
         [-0.8736],
         [-0.8788],
         [-0.8828],
         [-0.8863]],

        [[-0.7842],
         [-0.7860],
         [-0.8016],
         [-0.8044],
         [-0.8084],
         [-0.8136],
         [-0.8196]],

        [[-0.8954],
         [-0.8936],
         [-0.8998],
         [-0.9022],
         [-0.9035],
         [-0.9077],
         [-0.9115]],

        [[-0.7569],
         [-0.7481],
         [-0.7510],
         [-0.7532],
         [-0.7577],
         [-0.7681],
         [-0.7804]],

        [[-0.5042],
         [-0.5442],
         [-0.5786],
         [-0.5705],
         [-0.5791],
         [-0.5887],
         [-0.5994]],

        [[-0.9252],
         [-0.9254],
         [-0.9315],
         [-0.9333],
         [-0.9352],
         [-0.9381],
         [-0.9409]],

        [[-0.9983],
         [-0.9979],
         [-0.9980],
         [-0.9980],
         [-0.9977],
         [-0.9978],
         [-0.9982]],

        [[-0.6721],
         [-0.6719],
         [-0.6816],
         [-0.6910],
         [-0.7077],
         [-0.7195],
         [-0.7335]],

        [[-0.8140],
         [-0.8193],
         [-0.8260],
         [-0.8285],
         [-0.8402],
         [-0.8469],
         [-0.8526]],

        [[-0.8182],
         [-0.8115],
         [-0.8181],
         [-0.8204],
         [-0.8286],
         [-0.8375],
         [-0.8475]],

        [[-0.8601],
         [-0.8553],
         [-0.8529],
         [-0.8584],
         [-0.8652],
         [-0.8710],
         [-0.8763]],

        [[-0.8636],
         [-0.8579],
         [-0.8641],
         [-0.8686],
         [-0.8730],
         [-0.8780],
         [-0.8810]],

        [[-0.6343],
         [-0.6238],
         [-0.6411],
         [-0.6486],
         [-0.6655],
         [-0.6838],
         [-0.6984]],

        [[-0.9650],
         [-0.9678],
         [-0.9694],
         [-0.9708],
         [-0.9727],
         [-0.9742],
         [-0.9748]],

        [[-0.4969],
         [-0.5010],
         [-0.4945],
         [-0.5093],
         [-0.5244],
         [-0.5484],
         [-0.5674]]], device='npu:0')
########




#########
tensor([[[-0.8689],
         [-0.8615],
         [-0.8699],
         [-0.8736],
         [-0.8788],
         [-0.8828],
         [-0.8863],
         [-0.8904]],

        [[-0.7842],
         [-0.7860],
         [-0.8016],
         [-0.8044],
         [-0.8084],
         [-0.8136],
         [-0.8196],
         [-0.8248]],

        [[-0.8954],
         [-0.8936],
         [-0.8998],
         [-0.9022],
         [-0.9035],
         [-0.9077],
         [-0.9115],
         [-0.9146]],

        [[-0.7569],
         [-0.7481],
         [-0.7510],
         [-0.7532],
         [-0.7577],
         [-0.7681],
         [-0.7804],
         [-0.7952]],

        [[-0.5042],
         [-0.5442],
         [-0.5786],
         [-0.5705],
         [-0.5791],
         [-0.5887],
         [-0.5994],
         [-0.6106]],

        [[-0.9252],
         [-0.9254],
         [-0.9315],
         [-0.9333],
         [-0.9352],
         [-0.9381],
         [-0.9409],
         [-0.9442]],

        [[-0.9983],
         [-0.9979],
         [-0.9980],
         [-0.9980],
         [-0.9977],
         [-0.9978],
         [-0.9982],
         [-0.9979]],

        [[-0.6721],
         [-0.6719],
         [-0.6816],
         [-0.6910],
         [-0.7077],
         [-0.7195],
         [-0.7335],
         [-0.7473]],

        [[-0.8140],
         [-0.8193],
         [-0.8260],
         [-0.8285],
         [-0.8402],
         [-0.8469],
         [-0.8526],
         [-0.8587]],

        [[-0.8182],
         [-0.8115],
         [-0.8181],
         [-0.8204],
         [-0.8286],
         [-0.8375],
         [-0.8475],
         [-0.8575]],

        [[-0.8601],
         [-0.8553],
         [-0.8529],
         [-0.8584],
         [-0.8652],
         [-0.8710],
         [-0.8763],
         [-0.8821]],

        [[-0.8636],
         [-0.8579],
         [-0.8641],
         [-0.8686],
         [-0.8730],
         [-0.8780],
         [-0.8810],
         [-0.8854]],

        [[-0.6343],
         [-0.6238],
         [-0.6411],
         [-0.6486],
         [-0.6655],
         [-0.6838],
         [-0.6984],
         [-0.7164]],

        [[-0.9650],
         [-0.9678],
         [-0.9694],
         [-0.9708],
         [-0.9727],
         [-0.9742],
         [-0.9748],
         [-0.9755]],

        [[-0.4969],
         [-0.5010],
         [-0.4945],
         [-0.5093],
         [-0.5244],
         [-0.5484],
         [-0.5674],
         [-0.5911]]], device='npu:0')
########




#########
tensor([[[-0.8689],
         [-0.8615],
         [-0.8699],
         [-0.8736],
         [-0.8788],
         [-0.8828],
         [-0.8863],
         [-0.8904],
         [-0.8955]],

        [[-0.7842],
         [-0.7860],
         [-0.8016],
         [-0.8044],
         [-0.8084],
         [-0.8136],
         [-0.8196],
         [-0.8248],
         [-0.8304]],

        [[-0.8954],
         [-0.8936],
         [-0.8998],
         [-0.9022],
         [-0.9035],
         [-0.9077],
         [-0.9115],
         [-0.9146],
         [-0.9181]],

        [[-0.7569],
         [-0.7481],
         [-0.7510],
         [-0.7532],
         [-0.7577],
         [-0.7681],
         [-0.7804],
         [-0.7952],
         [-0.8108]],

        [[-0.5042],
         [-0.5442],
         [-0.5786],
         [-0.5705],
         [-0.5791],
         [-0.5887],
         [-0.5994],
         [-0.6106],
         [-0.6250]],

        [[-0.9252],
         [-0.9254],
         [-0.9315],
         [-0.9333],
         [-0.9352],
         [-0.9381],
         [-0.9409],
         [-0.9442],
         [-0.9480]],

        [[-0.9983],
         [-0.9979],
         [-0.9980],
         [-0.9980],
         [-0.9977],
         [-0.9978],
         [-0.9982],
         [-0.9979],
         [-0.9975]],

        [[-0.6721],
         [-0.6719],
         [-0.6816],
         [-0.6910],
         [-0.7077],
         [-0.7195],
         [-0.7335],
         [-0.7473],
         [-0.7668]],

        [[-0.8140],
         [-0.8193],
         [-0.8260],
         [-0.8285],
         [-0.8402],
         [-0.8469],
         [-0.8526],
         [-0.8587],
         [-0.8672]],

        [[-0.8182],
         [-0.8115],
         [-0.8181],
         [-0.8204],
         [-0.8286],
         [-0.8375],
         [-0.8475],
         [-0.8575],
         [-0.8647]],

        [[-0.8601],
         [-0.8553],
         [-0.8529],
         [-0.8584],
         [-0.8652],
         [-0.8710],
         [-0.8763],
         [-0.8821],
         [-0.8892]],

        [[-0.8636],
         [-0.8579],
         [-0.8641],
         [-0.8686],
         [-0.8730],
         [-0.8780],
         [-0.8810],
         [-0.8854],
         [-0.8902]],

        [[-0.6343],
         [-0.6238],
         [-0.6411],
         [-0.6486],
         [-0.6655],
         [-0.6838],
         [-0.6984],
         [-0.7164],
         [-0.7422]],

        [[-0.9650],
         [-0.9678],
         [-0.9694],
         [-0.9708],
         [-0.9727],
         [-0.9742],
         [-0.9748],
         [-0.9755],
         [-0.9768]],

        [[-0.4969],
         [-0.5010],
         [-0.4945],
         [-0.5093],
         [-0.5244],
         [-0.5484],
         [-0.5674],
         [-0.5911],
         [-0.6138]]], device='npu:0')
########




#########
tensor([[[-0.8689],
         [-0.8615],
         [-0.8699],
         [-0.8736],
         [-0.8788],
         [-0.8828],
         [-0.8863],
         [-0.8904],
         [-0.8955],
         [-0.9012]],

        [[-0.7842],
         [-0.7860],
         [-0.8016],
         [-0.8044],
         [-0.8084],
         [-0.8136],
         [-0.8196],
         [-0.8248],
         [-0.8304],
         [-0.8365]],

        [[-0.8954],
         [-0.8936],
         [-0.8998],
         [-0.9022],
         [-0.9035],
         [-0.9077],
         [-0.9115],
         [-0.9146],
         [-0.9181],
         [-0.9219]],

        [[-0.7569],
         [-0.7481],
         [-0.7510],
         [-0.7532],
         [-0.7577],
         [-0.7681],
         [-0.7804],
         [-0.7952],
         [-0.8108],
         [-0.8216]],

        [[-0.5042],
         [-0.5442],
         [-0.5786],
         [-0.5705],
         [-0.5791],
         [-0.5887],
         [-0.5994],
         [-0.6106],
         [-0.6250],
         [-0.6405]],

        [[-0.9252],
         [-0.9254],
         [-0.9315],
         [-0.9333],
         [-0.9352],
         [-0.9381],
         [-0.9409],
         [-0.9442],
         [-0.9480],
         [-0.9513]],

        [[-0.9983],
         [-0.9979],
         [-0.9980],
         [-0.9980],
         [-0.9977],
         [-0.9978],
         [-0.9982],
         [-0.9979],
         [-0.9975],
         [-0.9979]],

        [[-0.6721],
         [-0.6719],
         [-0.6816],
         [-0.6910],
         [-0.7077],
         [-0.7195],
         [-0.7335],
         [-0.7473],
         [-0.7668],
         [-0.7834]],

        [[-0.8140],
         [-0.8193],
         [-0.8260],
         [-0.8285],
         [-0.8402],
         [-0.8469],
         [-0.8526],
         [-0.8587],
         [-0.8672],
         [-0.8750]],

        [[-0.8182],
         [-0.8115],
         [-0.8181],
         [-0.8204],
         [-0.8286],
         [-0.8375],
         [-0.8475],
         [-0.8575],
         [-0.8647],
         [-0.8710]],

        [[-0.8601],
         [-0.8553],
         [-0.8529],
         [-0.8584],
         [-0.8652],
         [-0.8710],
         [-0.8763],
         [-0.8821],
         [-0.8892],
         [-0.8956]],

        [[-0.8636],
         [-0.8579],
         [-0.8641],
         [-0.8686],
         [-0.8730],
         [-0.8780],
         [-0.8810],
         [-0.8854],
         [-0.8902],
         [-0.8948]],

        [[-0.6343],
         [-0.6238],
         [-0.6411],
         [-0.6486],
         [-0.6655],
         [-0.6838],
         [-0.6984],
         [-0.7164],
         [-0.7422],
         [-0.7608]],

        [[-0.9650],
         [-0.9678],
         [-0.9694],
         [-0.9708],
         [-0.9727],
         [-0.9742],
         [-0.9748],
         [-0.9755],
         [-0.9768],
         [-0.9780]],

        [[-0.4969],
         [-0.5010],
         [-0.4945],
         [-0.5093],
         [-0.5244],
         [-0.5484],
         [-0.5674],
         [-0.5911],
         [-0.6138],
         [-0.6329]]], device='npu:0')
########




#########
tensor([[[-0.8689],
         [-0.8615],
         [-0.8699],
         [-0.8736],
         [-0.8788],
         [-0.8828],
         [-0.8863],
         [-0.8904],
         [-0.8955],
         [-0.9012],
         [-0.9085]],

        [[-0.7842],
         [-0.7860],
         [-0.8016],
         [-0.8044],
         [-0.8084],
         [-0.8136],
         [-0.8196],
         [-0.8248],
         [-0.8304],
         [-0.8365],
         [-0.8422]],

        [[-0.8954],
         [-0.8936],
         [-0.8998],
         [-0.9022],
         [-0.9035],
         [-0.9077],
         [-0.9115],
         [-0.9146],
         [-0.9181],
         [-0.9219],
         [-0.9262]],

        [[-0.7569],
         [-0.7481],
         [-0.7510],
         [-0.7532],
         [-0.7577],
         [-0.7681],
         [-0.7804],
         [-0.7952],
         [-0.8108],
         [-0.8216],
         [-0.8300]],

        [[-0.5042],
         [-0.5442],
         [-0.5786],
         [-0.5705],
         [-0.5791],
         [-0.5887],
         [-0.5994],
         [-0.6106],
         [-0.6250],
         [-0.6405],
         [-0.6627]],

        [[-0.9252],
         [-0.9254],
         [-0.9315],
         [-0.9333],
         [-0.9352],
         [-0.9381],
         [-0.9409],
         [-0.9442],
         [-0.9480],
         [-0.9513],
         [-0.9542]],

        [[-0.9983],
         [-0.9979],
         [-0.9980],
         [-0.9980],
         [-0.9977],
         [-0.9978],
         [-0.9982],
         [-0.9979],
         [-0.9975],
         [-0.9979],
         [-0.9982]],

        [[-0.6721],
         [-0.6719],
         [-0.6816],
         [-0.6910],
         [-0.7077],
         [-0.7195],
         [-0.7335],
         [-0.7473],
         [-0.7668],
         [-0.7834],
         [-0.7927]],

        [[-0.8140],
         [-0.8193],
         [-0.8260],
         [-0.8285],
         [-0.8402],
         [-0.8469],
         [-0.8526],
         [-0.8587],
         [-0.8672],
         [-0.8750],
         [-0.8826]],

        [[-0.8182],
         [-0.8115],
         [-0.8181],
         [-0.8204],
         [-0.8286],
         [-0.8375],
         [-0.8475],
         [-0.8575],
         [-0.8647],
         [-0.8710],
         [-0.8772]],

        [[-0.8601],
         [-0.8553],
         [-0.8529],
         [-0.8584],
         [-0.8652],
         [-0.8710],
         [-0.8763],
         [-0.8821],
         [-0.8892],
         [-0.8956],
         [-0.9019]],

        [[-0.8636],
         [-0.8579],
         [-0.8641],
         [-0.8686],
         [-0.8730],
         [-0.8780],
         [-0.8810],
         [-0.8854],
         [-0.8902],
         [-0.8948],
         [-0.9007]],

        [[-0.6343],
         [-0.6238],
         [-0.6411],
         [-0.6486],
         [-0.6655],
         [-0.6838],
         [-0.6984],
         [-0.7164],
         [-0.7422],
         [-0.7608],
         [-0.7734]],

        [[-0.9650],
         [-0.9678],
         [-0.9694],
         [-0.9708],
         [-0.9727],
         [-0.9742],
         [-0.9748],
         [-0.9755],
         [-0.9768],
         [-0.9780],
         [-0.9792]],

        [[-0.4969],
         [-0.5010],
         [-0.4945],
         [-0.5093],
         [-0.5244],
         [-0.5484],
         [-0.5674],
         [-0.5911],
         [-0.6138],
         [-0.6329],
         [-0.6511]]], device='npu:0')
########




#########
tensor([[[-0.8689],
         [-0.8615],
         [-0.8699],
         [-0.8736],
         [-0.8788],
         [-0.8828],
         [-0.8863],
         [-0.8904],
         [-0.8955],
         [-0.9012],
         [-0.9085],
         [-0.9151]],

        [[-0.7842],
         [-0.7860],
         [-0.8016],
         [-0.8044],
         [-0.8084],
         [-0.8136],
         [-0.8196],
         [-0.8248],
         [-0.8304],
         [-0.8365],
         [-0.8422],
         [-0.8486]],

        [[-0.8954],
         [-0.8936],
         [-0.8998],
         [-0.9022],
         [-0.9035],
         [-0.9077],
         [-0.9115],
         [-0.9146],
         [-0.9181],
         [-0.9219],
         [-0.9262],
         [-0.9321]],

        [[-0.7569],
         [-0.7481],
         [-0.7510],
         [-0.7532],
         [-0.7577],
         [-0.7681],
         [-0.7804],
         [-0.7952],
         [-0.8108],
         [-0.8216],
         [-0.8300],
         [-0.8380]],

        [[-0.5042],
         [-0.5442],
         [-0.5786],
         [-0.5705],
         [-0.5791],
         [-0.5887],
         [-0.5994],
         [-0.6106],
         [-0.6250],
         [-0.6405],
         [-0.6627],
         [-0.6943]],

        [[-0.9252],
         [-0.9254],
         [-0.9315],
         [-0.9333],
         [-0.9352],
         [-0.9381],
         [-0.9409],
         [-0.9442],
         [-0.9480],
         [-0.9513],
         [-0.9542],
         [-0.9571]],

        [[-0.9983],
         [-0.9979],
         [-0.9980],
         [-0.9980],
         [-0.9977],
         [-0.9978],
         [-0.9982],
         [-0.9979],
         [-0.9975],
         [-0.9979],
         [-0.9982],
         [-0.9980]],

        [[-0.6721],
         [-0.6719],
         [-0.6816],
         [-0.6910],
         [-0.7077],
         [-0.7195],
         [-0.7335],
         [-0.7473],
         [-0.7668],
         [-0.7834],
         [-0.7927],
         [-0.8028]],

        [[-0.8140],
         [-0.8193],
         [-0.8260],
         [-0.8285],
         [-0.8402],
         [-0.8469],
         [-0.8526],
         [-0.8587],
         [-0.8672],
         [-0.8750],
         [-0.8826],
         [-0.8925]],

        [[-0.8182],
         [-0.8115],
         [-0.8181],
         [-0.8204],
         [-0.8286],
         [-0.8375],
         [-0.8475],
         [-0.8575],
         [-0.8647],
         [-0.8710],
         [-0.8772],
         [-0.8837]],

        [[-0.8601],
         [-0.8553],
         [-0.8529],
         [-0.8584],
         [-0.8652],
         [-0.8710],
         [-0.8763],
         [-0.8821],
         [-0.8892],
         [-0.8956],
         [-0.9019],
         [-0.9090]],

        [[-0.8636],
         [-0.8579],
         [-0.8641],
         [-0.8686],
         [-0.8730],
         [-0.8780],
         [-0.8810],
         [-0.8854],
         [-0.8902],
         [-0.8948],
         [-0.9007],
         [-0.9077]],

        [[-0.6343],
         [-0.6238],
         [-0.6411],
         [-0.6486],
         [-0.6655],
         [-0.6838],
         [-0.6984],
         [-0.7164],
         [-0.7422],
         [-0.7608],
         [-0.7734],
         [-0.7895]],

        [[-0.9650],
         [-0.9678],
         [-0.9694],
         [-0.9708],
         [-0.9727],
         [-0.9742],
         [-0.9748],
         [-0.9755],
         [-0.9768],
         [-0.9780],
         [-0.9792],
         [-0.9799]],

        [[-0.4969],
         [-0.5010],
         [-0.4945],
         [-0.5093],
         [-0.5244],
         [-0.5484],
         [-0.5674],
         [-0.5911],
         [-0.6138],
         [-0.6329],
         [-0.6511],
         [-0.6714]]], device='npu:0')
########


金湖东路-乐安街未来1h车流预测为:
557.71686;563.5051;538.7696;516.25226;495.4317;473.54684;461.29956;448.00525;431.56183;419.87836;409.94373;398.77133
2023-11-09 13:48:28 [INFO] 127.0.0.1 - - [09/Nov/2023 13:48:28] "POST /TrafficFlowForcast HTTP/1.1" 200 -
!!!!!!!!1
CUDA: True cuda:0
Read configuration file: configurations/PREDICT.conf
graph_signal_matrix_filenamedata/PREDICT/20231101.npy
!!!!num_of_hours 1
!!!!num_of_days 0
!!!!num_of_hours 1
!!!!num_of_weeks 0
!!!!DEVICE cuda:0
!!!!batch_size 12
file: 20231101
load file: data/PREDICT/20231101_r1_d0_w0.npz
ori length: 288 , percent: 1.0 , scale: 288
text_x shape: (288, 15, 3, 12)
after cut text_x shape: (12, 15, 3, 12)
train: torch.Size([288, 15, 1, 12]) torch.Size([288, 15, 12]) torch.Size([288, 15, 12])
val: torch.Size([288, 15, 1, 12]) torch.Size([288, 15, 12]) torch.Size([288, 15, 12])
test: torch.Size([12, 15, 1, 12]) torch.Size([12, 15, 12]) torch.Size([12, 15, 12])
!!!!!!!2
!!!!!!!!3

load weight from: ./experiments/PREDICT/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TE/epoch_100.params


#########
tensor([[[-0.8689]],

        [[-0.7842]],

        [[-0.8954]],

        [[-0.7569]],

        [[-0.5042]],

        [[-0.9252]],

        [[-0.9983]],

        [[-0.6721]],

        [[-0.8140]],

        [[-0.8182]],

        [[-0.8601]],

        [[-0.8636]],

        [[-0.6343]],

        [[-0.9650]],

        [[-0.4969]]], device='npu:0')
########




#########
tensor([[[-0.8689],
         [-0.8615]],

        [[-0.7842],
         [-0.7860]],

        [[-0.8954],
         [-0.8936]],

        [[-0.7569],
         [-0.7481]],

        [[-0.5042],
         [-0.5442]],

        [[-0.9252],
         [-0.9254]],

        [[-0.9983],
         [-0.9979]],

        [[-0.6721],
         [-0.6719]],

        [[-0.8140],
         [-0.8193]],

        [[-0.8182],
         [-0.8115]],

        [[-0.8601],
         [-0.8553]],

        [[-0.8636],
         [-0.8579]],

        [[-0.6343],
         [-0.6238]],

        [[-0.9650],
         [-0.9678]],

        [[-0.4969],
         [-0.5010]]], device='npu:0')
########




#########
tensor([[[-0.8689],
         [-0.8615],
         [-0.8699]],

        [[-0.7842],
         [-0.7860],
         [-0.8016]],

        [[-0.8954],
         [-0.8936],
         [-0.8998]],

        [[-0.7569],
         [-0.7481],
         [-0.7510]],

        [[-0.5042],
         [-0.5442],
         [-0.5786]],

        [[-0.9252],
         [-0.9254],
         [-0.9315]],

        [[-0.9983],
         [-0.9979],
         [-0.9980]],

        [[-0.6721],
         [-0.6719],
         [-0.6816]],

        [[-0.8140],
         [-0.8193],
         [-0.8260]],

        [[-0.8182],
         [-0.8115],
         [-0.8181]],

        [[-0.8601],
         [-0.8553],
         [-0.8529]],

        [[-0.8636],
         [-0.8579],
         [-0.8641]],

        [[-0.6343],
         [-0.6238],
         [-0.6411]],

        [[-0.9650],
         [-0.9678],
         [-0.9694]],

        [[-0.4969],
         [-0.5010],
         [-0.4945]]], device='npu:0')
########




#########
tensor([[[-0.8689],
         [-0.8615],
         [-0.8699],
         [-0.8736]],

        [[-0.7842],
         [-0.7860],
         [-0.8016],
         [-0.8044]],

        [[-0.8954],
         [-0.8936],
         [-0.8998],
         [-0.9022]],

        [[-0.7569],
         [-0.7481],
         [-0.7510],
         [-0.7532]],

        [[-0.5042],
         [-0.5442],
         [-0.5786],
         [-0.5705]],

        [[-0.9252],
         [-0.9254],
         [-0.9315],
         [-0.9333]],

        [[-0.9983],
         [-0.9979],
         [-0.9980],
         [-0.9980]],

        [[-0.6721],
         [-0.6719],
         [-0.6816],
         [-0.6910]],

        [[-0.8140],
         [-0.8193],
         [-0.8260],
         [-0.8285]],

        [[-0.8182],
         [-0.8115],
         [-0.8181],
         [-0.8204]],

        [[-0.8601],
         [-0.8553],
         [-0.8529],
         [-0.8584]],

        [[-0.8636],
         [-0.8579],
         [-0.8641],
         [-0.8686]],

        [[-0.6343],
         [-0.6238],
         [-0.6411],
         [-0.6486]],

        [[-0.9650],
         [-0.9678],
         [-0.9694],
         [-0.9708]],

        [[-0.4969],
         [-0.5010],
         [-0.4945],
         [-0.5093]]], device='npu:0')
########




#########
tensor([[[-0.8689],
         [-0.8615],
         [-0.8699],
         [-0.8736],
         [-0.8788]],

        [[-0.7842],
         [-0.7860],
         [-0.8016],
         [-0.8044],
         [-0.8084]],

        [[-0.8954],
         [-0.8936],
         [-0.8998],
         [-0.9022],
         [-0.9035]],

        [[-0.7569],
         [-0.7481],
         [-0.7510],
         [-0.7532],
         [-0.7577]],

        [[-0.5042],
         [-0.5442],
         [-0.5786],
         [-0.5705],
         [-0.5791]],

        [[-0.9252],
         [-0.9254],
         [-0.9315],
         [-0.9333],
         [-0.9352]],

        [[-0.9983],
         [-0.9979],
         [-0.9980],
         [-0.9980],
         [-0.9977]],

        [[-0.6721],
         [-0.6719],
         [-0.6816],
         [-0.6910],
         [-0.7077]],

        [[-0.8140],
         [-0.8193],
         [-0.8260],
         [-0.8285],
         [-0.8402]],

        [[-0.8182],
         [-0.8115],
         [-0.8181],
         [-0.8204],
         [-0.8286]],

        [[-0.8601],
         [-0.8553],
         [-0.8529],
         [-0.8584],
         [-0.8652]],

        [[-0.8636],
         [-0.8579],
         [-0.8641],
         [-0.8686],
         [-0.8730]],

        [[-0.6343],
         [-0.6238],
         [-0.6411],
         [-0.6486],
         [-0.6655]],

        [[-0.9650],
         [-0.9678],
         [-0.9694],
         [-0.9708],
         [-0.9727]],

        [[-0.4969],
         [-0.5010],
         [-0.4945],
         [-0.5093],
         [-0.5244]]], device='npu:0')
########




#########
tensor([[[-0.8689],
         [-0.8615],
         [-0.8699],
         [-0.8736],
         [-0.8788],
         [-0.8828]],

        [[-0.7842],
         [-0.7860],
         [-0.8016],
         [-0.8044],
         [-0.8084],
         [-0.8136]],

        [[-0.8954],
         [-0.8936],
         [-0.8998],
         [-0.9022],
         [-0.9035],
         [-0.9077]],

        [[-0.7569],
         [-0.7481],
         [-0.7510],
         [-0.7532],
         [-0.7577],
         [-0.7681]],

        [[-0.5042],
         [-0.5442],
         [-0.5786],
         [-0.5705],
         [-0.5791],
         [-0.5887]],

        [[-0.9252],
         [-0.9254],
         [-0.9315],
         [-0.9333],
         [-0.9352],
         [-0.9381]],

        [[-0.9983],
         [-0.9979],
         [-0.9980],
         [-0.9980],
         [-0.9977],
         [-0.9978]],

        [[-0.6721],
         [-0.6719],
         [-0.6816],
         [-0.6910],
         [-0.7077],
         [-0.7195]],

        [[-0.8140],
         [-0.8193],
         [-0.8260],
         [-0.8285],
         [-0.8402],
         [-0.8469]],

        [[-0.8182],
         [-0.8115],
         [-0.8181],
         [-0.8204],
         [-0.8286],
         [-0.8375]],

        [[-0.8601],
         [-0.8553],
         [-0.8529],
         [-0.8584],
         [-0.8652],
         [-0.8710]],

        [[-0.8636],
         [-0.8579],
         [-0.8641],
         [-0.8686],
         [-0.8730],
         [-0.8780]],

        [[-0.6343],
         [-0.6238],
         [-0.6411],
         [-0.6486],
         [-0.6655],
         [-0.6838]],

        [[-0.9650],
         [-0.9678],
         [-0.9694],
         [-0.9708],
         [-0.9727],
         [-0.9742]],

        [[-0.4969],
         [-0.5010],
         [-0.4945],
         [-0.5093],
         [-0.5244],
         [-0.5484]]], device='npu:0')
########




#########
tensor([[[-0.8689],
         [-0.8615],
         [-0.8699],
         [-0.8736],
         [-0.8788],
         [-0.8828],
         [-0.8863]],

        [[-0.7842],
         [-0.7860],
         [-0.8016],
         [-0.8044],
         [-0.8084],
         [-0.8136],
         [-0.8196]],

        [[-0.8954],
         [-0.8936],
         [-0.8998],
         [-0.9022],
         [-0.9035],
         [-0.9077],
         [-0.9115]],

        [[-0.7569],
         [-0.7481],
         [-0.7510],
         [-0.7532],
         [-0.7577],
         [-0.7681],
         [-0.7804]],

        [[-0.5042],
         [-0.5442],
         [-0.5786],
         [-0.5705],
         [-0.5791],
         [-0.5887],
         [-0.5994]],

        [[-0.9252],
         [-0.9254],
         [-0.9315],
         [-0.9333],
         [-0.9352],
         [-0.9381],
         [-0.9409]],

        [[-0.9983],
         [-0.9979],
         [-0.9980],
         [-0.9980],
         [-0.9977],
         [-0.9978],
         [-0.9982]],

        [[-0.6721],
         [-0.6719],
         [-0.6816],
         [-0.6910],
         [-0.7077],
         [-0.7195],
         [-0.7335]],

        [[-0.8140],
         [-0.8193],
         [-0.8260],
         [-0.8285],
         [-0.8402],
         [-0.8469],
         [-0.8526]],

        [[-0.8182],
         [-0.8115],
         [-0.8181],
         [-0.8204],
         [-0.8286],
         [-0.8375],
         [-0.8475]],

        [[-0.8601],
         [-0.8553],
         [-0.8529],
         [-0.8584],
         [-0.8652],
         [-0.8710],
         [-0.8763]],

        [[-0.8636],
         [-0.8579],
         [-0.8641],
         [-0.8686],
         [-0.8730],
         [-0.8780],
         [-0.8810]],

        [[-0.6343],
         [-0.6238],
         [-0.6411],
         [-0.6486],
         [-0.6655],
         [-0.6838],
         [-0.6984]],

        [[-0.9650],
         [-0.9678],
         [-0.9694],
         [-0.9708],
         [-0.9727],
         [-0.9742],
         [-0.9748]],

        [[-0.4969],
         [-0.5010],
         [-0.4945],
         [-0.5093],
         [-0.5244],
         [-0.5484],
         [-0.5674]]], device='npu:0')
########




#########
tensor([[[-0.8689],
         [-0.8615],
         [-0.8699],
         [-0.8736],
         [-0.8788],
         [-0.8828],
         [-0.8863],
         [-0.8904]],

        [[-0.7842],
         [-0.7860],
         [-0.8016],
         [-0.8044],
         [-0.8084],
         [-0.8136],
         [-0.8196],
         [-0.8248]],

        [[-0.8954],
         [-0.8936],
         [-0.8998],
         [-0.9022],
         [-0.9035],
         [-0.9077],
         [-0.9115],
         [-0.9146]],

        [[-0.7569],
         [-0.7481],
         [-0.7510],
         [-0.7532],
         [-0.7577],
         [-0.7681],
         [-0.7804],
         [-0.7952]],

        [[-0.5042],
         [-0.5442],
         [-0.5786],
         [-0.5705],
         [-0.5791],
         [-0.5887],
         [-0.5994],
         [-0.6106]],

        [[-0.9252],
         [-0.9254],
         [-0.9315],
         [-0.9333],
         [-0.9352],
         [-0.9381],
         [-0.9409],
         [-0.9442]],

        [[-0.9983],
         [-0.9979],
         [-0.9980],
         [-0.9980],
         [-0.9977],
         [-0.9978],
         [-0.9982],
         [-0.9979]],

        [[-0.6721],
         [-0.6719],
         [-0.6816],
         [-0.6910],
         [-0.7077],
         [-0.7195],
         [-0.7335],
         [-0.7473]],

        [[-0.8140],
         [-0.8193],
         [-0.8260],
         [-0.8285],
         [-0.8402],
         [-0.8469],
         [-0.8526],
         [-0.8587]],

        [[-0.8182],
         [-0.8115],
         [-0.8181],
         [-0.8204],
         [-0.8286],
         [-0.8375],
         [-0.8475],
         [-0.8575]],

        [[-0.8601],
         [-0.8553],
         [-0.8529],
         [-0.8584],
         [-0.8652],
         [-0.8710],
         [-0.8763],
         [-0.8821]],

        [[-0.8636],
         [-0.8579],
         [-0.8641],
         [-0.8686],
         [-0.8730],
         [-0.8780],
         [-0.8810],
         [-0.8854]],

        [[-0.6343],
         [-0.6238],
         [-0.6411],
         [-0.6486],
         [-0.6655],
         [-0.6838],
         [-0.6984],
         [-0.7164]],

        [[-0.9650],
         [-0.9678],
         [-0.9694],
         [-0.9708],
         [-0.9727],
         [-0.9742],
         [-0.9748],
         [-0.9755]],

        [[-0.4969],
         [-0.5010],
         [-0.4945],
         [-0.5093],
         [-0.5244],
         [-0.5484],
         [-0.5674],
         [-0.5911]]], device='npu:0')
########




#########
tensor([[[-0.8689],
         [-0.8615],
         [-0.8699],
         [-0.8736],
         [-0.8788],
         [-0.8828],
         [-0.8863],
         [-0.8904],
         [-0.8955]],

        [[-0.7842],
         [-0.7860],
         [-0.8016],
         [-0.8044],
         [-0.8084],
         [-0.8136],
         [-0.8196],
         [-0.8248],
         [-0.8304]],

        [[-0.8954],
         [-0.8936],
         [-0.8998],
         [-0.9022],
         [-0.9035],
         [-0.9077],
         [-0.9115],
         [-0.9146],
         [-0.9181]],

        [[-0.7569],
         [-0.7481],
         [-0.7510],
         [-0.7532],
         [-0.7577],
         [-0.7681],
         [-0.7804],
         [-0.7952],
         [-0.8108]],

        [[-0.5042],
         [-0.5442],
         [-0.5786],
         [-0.5705],
         [-0.5791],
         [-0.5887],
         [-0.5994],
         [-0.6106],
         [-0.6250]],

        [[-0.9252],
         [-0.9254],
         [-0.9315],
         [-0.9333],
         [-0.9352],
         [-0.9381],
         [-0.9409],
         [-0.9442],
         [-0.9480]],

        [[-0.9983],
         [-0.9979],
         [-0.9980],
         [-0.9980],
         [-0.9977],
         [-0.9978],
         [-0.9982],
         [-0.9979],
         [-0.9975]],

        [[-0.6721],
         [-0.6719],
         [-0.6816],
         [-0.6910],
         [-0.7077],
         [-0.7195],
         [-0.7335],
         [-0.7473],
         [-0.7668]],

        [[-0.8140],
         [-0.8193],
         [-0.8260],
         [-0.8285],
         [-0.8402],
         [-0.8469],
         [-0.8526],
         [-0.8587],
         [-0.8672]],

        [[-0.8182],
         [-0.8115],
         [-0.8181],
         [-0.8204],
         [-0.8286],
         [-0.8375],
         [-0.8475],
         [-0.8575],
         [-0.8647]],

        [[-0.8601],
         [-0.8553],
         [-0.8529],
         [-0.8584],
         [-0.8652],
         [-0.8710],
         [-0.8763],
         [-0.8821],
         [-0.8892]],

        [[-0.8636],
         [-0.8579],
         [-0.8641],
         [-0.8686],
         [-0.8730],
         [-0.8780],
         [-0.8810],
         [-0.8854],
         [-0.8902]],

        [[-0.6343],
         [-0.6238],
         [-0.6411],
         [-0.6486],
         [-0.6655],
         [-0.6838],
         [-0.6984],
         [-0.7164],
         [-0.7422]],

        [[-0.9650],
         [-0.9678],
         [-0.9694],
         [-0.9708],
         [-0.9727],
         [-0.9742],
         [-0.9748],
         [-0.9755],
         [-0.9768]],

        [[-0.4969],
         [-0.5010],
         [-0.4945],
         [-0.5093],
         [-0.5244],
         [-0.5484],
         [-0.5674],
         [-0.5911],
         [-0.6138]]], device='npu:0')
########




#########
tensor([[[-0.8689],
         [-0.8615],
         [-0.8699],
         [-0.8736],
         [-0.8788],
         [-0.8828],
         [-0.8863],
         [-0.8904],
         [-0.8955],
         [-0.9012]],

        [[-0.7842],
         [-0.7860],
         [-0.8016],
         [-0.8044],
         [-0.8084],
         [-0.8136],
         [-0.8196],
         [-0.8248],
         [-0.8304],
         [-0.8365]],

        [[-0.8954],
         [-0.8936],
         [-0.8998],
         [-0.9022],
         [-0.9035],
         [-0.9077],
         [-0.9115],
         [-0.9146],
         [-0.9181],
         [-0.9219]],

        [[-0.7569],
         [-0.7481],
         [-0.7510],
         [-0.7532],
         [-0.7577],
         [-0.7681],
         [-0.7804],
         [-0.7952],
         [-0.8108],
         [-0.8216]],

        [[-0.5042],
         [-0.5442],
         [-0.5786],
         [-0.5705],
         [-0.5791],
         [-0.5887],
         [-0.5994],
         [-0.6106],
         [-0.6250],
         [-0.6405]],

        [[-0.9252],
         [-0.9254],
         [-0.9315],
         [-0.9333],
         [-0.9352],
         [-0.9381],
         [-0.9409],
         [-0.9442],
         [-0.9480],
         [-0.9513]],

        [[-0.9983],
         [-0.9979],
         [-0.9980],
         [-0.9980],
         [-0.9977],
         [-0.9978],
         [-0.9982],
         [-0.9979],
         [-0.9975],
         [-0.9979]],

        [[-0.6721],
         [-0.6719],
         [-0.6816],
         [-0.6910],
         [-0.7077],
         [-0.7195],
         [-0.7335],
         [-0.7473],
         [-0.7668],
         [-0.7834]],

        [[-0.8140],
         [-0.8193],
         [-0.8260],
         [-0.8285],
         [-0.8402],
         [-0.8469],
         [-0.8526],
         [-0.8587],
         [-0.8672],
         [-0.8750]],

        [[-0.8182],
         [-0.8115],
         [-0.8181],
         [-0.8204],
         [-0.8286],
         [-0.8375],
         [-0.8475],
         [-0.8575],
         [-0.8647],
         [-0.8710]],

        [[-0.8601],
         [-0.8553],
         [-0.8529],
         [-0.8584],
         [-0.8652],
         [-0.8710],
         [-0.8763],
         [-0.8821],
         [-0.8892],
         [-0.8956]],

        [[-0.8636],
         [-0.8579],
         [-0.8641],
         [-0.8686],
         [-0.8730],
         [-0.8780],
         [-0.8810],
         [-0.8854],
         [-0.8902],
         [-0.8948]],

        [[-0.6343],
         [-0.6238],
         [-0.6411],
         [-0.6486],
         [-0.6655],
         [-0.6838],
         [-0.6984],
         [-0.7164],
         [-0.7422],
         [-0.7608]],

        [[-0.9650],
         [-0.9678],
         [-0.9694],
         [-0.9708],
         [-0.9727],
         [-0.9742],
         [-0.9748],
         [-0.9755],
         [-0.9768],
         [-0.9780]],

        [[-0.4969],
         [-0.5010],
         [-0.4945],
         [-0.5093],
         [-0.5244],
         [-0.5484],
         [-0.5674],
         [-0.5911],
         [-0.6138],
         [-0.6329]]], device='npu:0')
########




#########
tensor([[[-0.8689],
         [-0.8615],
         [-0.8699],
         [-0.8736],
         [-0.8788],
         [-0.8828],
         [-0.8863],
         [-0.8904],
         [-0.8955],
         [-0.9012],
         [-0.9085]],

        [[-0.7842],
         [-0.7860],
         [-0.8016],
         [-0.8044],
         [-0.8084],
         [-0.8136],
         [-0.8196],
         [-0.8248],
         [-0.8304],
         [-0.8365],
         [-0.8422]],

        [[-0.8954],
         [-0.8936],
         [-0.8998],
         [-0.9022],
         [-0.9035],
         [-0.9077],
         [-0.9115],
         [-0.9146],
         [-0.9181],
         [-0.9219],
         [-0.9262]],

        [[-0.7569],
         [-0.7481],
         [-0.7510],
         [-0.7532],
         [-0.7577],
         [-0.7681],
         [-0.7804],
         [-0.7952],
         [-0.8108],
         [-0.8216],
         [-0.8300]],

        [[-0.5042],
         [-0.5442],
         [-0.5786],
         [-0.5705],
         [-0.5791],
         [-0.5887],
         [-0.5994],
         [-0.6106],
         [-0.6250],
         [-0.6405],
         [-0.6627]],

        [[-0.9252],
         [-0.9254],
         [-0.9315],
         [-0.9333],
         [-0.9352],
         [-0.9381],
         [-0.9409],
         [-0.9442],
         [-0.9480],
         [-0.9513],
         [-0.9542]],

        [[-0.9983],
         [-0.9979],
         [-0.9980],
         [-0.9980],
         [-0.9977],
         [-0.9978],
         [-0.9982],
         [-0.9979],
         [-0.9975],
         [-0.9979],
         [-0.9982]],

        [[-0.6721],
         [-0.6719],
         [-0.6816],
         [-0.6910],
         [-0.7077],
         [-0.7195],
         [-0.7335],
         [-0.7473],
         [-0.7668],
         [-0.7834],
         [-0.7927]],

        [[-0.8140],
         [-0.8193],
         [-0.8260],
         [-0.8285],
         [-0.8402],
         [-0.8469],
         [-0.8526],
         [-0.8587],
         [-0.8672],
         [-0.8750],
         [-0.8826]],

        [[-0.8182],
         [-0.8115],
         [-0.8181],
         [-0.8204],
         [-0.8286],
         [-0.8375],
         [-0.8475],
         [-0.8575],
         [-0.8647],
         [-0.8710],
         [-0.8772]],

        [[-0.8601],
         [-0.8553],
         [-0.8529],
         [-0.8584],
         [-0.8652],
         [-0.8710],
         [-0.8763],
         [-0.8821],
         [-0.8892],
         [-0.8956],
         [-0.9019]],

        [[-0.8636],
         [-0.8579],
         [-0.8641],
         [-0.8686],
         [-0.8730],
         [-0.8780],
         [-0.8810],
         [-0.8854],
         [-0.8902],
         [-0.8948],
         [-0.9007]],

        [[-0.6343],
         [-0.6238],
         [-0.6411],
         [-0.6486],
         [-0.6655],
         [-0.6838],
         [-0.6984],
         [-0.7164],
         [-0.7422],
         [-0.7608],
         [-0.7734]],

        [[-0.9650],
         [-0.9678],
         [-0.9694],
         [-0.9708],
         [-0.9727],
         [-0.9742],
         [-0.9748],
         [-0.9755],
         [-0.9768],
         [-0.9780],
         [-0.9792]],

        [[-0.4969],
         [-0.5010],
         [-0.4945],
         [-0.5093],
         [-0.5244],
         [-0.5484],
         [-0.5674],
         [-0.5911],
         [-0.6138],
         [-0.6329],
         [-0.6511]]], device='npu:0')
########




#########
tensor([[[-0.8689],
         [-0.8615],
         [-0.8699],
         [-0.8736],
         [-0.8788],
         [-0.8828],
         [-0.8863],
         [-0.8904],
         [-0.8955],
         [-0.9012],
         [-0.9085],
         [-0.9151]],

        [[-0.7842],
         [-0.7860],
         [-0.8016],
         [-0.8044],
         [-0.8084],
         [-0.8136],
         [-0.8196],
         [-0.8248],
         [-0.8304],
         [-0.8365],
         [-0.8422],
         [-0.8486]],

        [[-0.8954],
         [-0.8936],
         [-0.8998],
         [-0.9022],
         [-0.9035],
         [-0.9077],
         [-0.9115],
         [-0.9146],
         [-0.9181],
         [-0.9219],
         [-0.9262],
         [-0.9321]],

        [[-0.7569],
         [-0.7481],
         [-0.7510],
         [-0.7532],
         [-0.7577],
         [-0.7681],
         [-0.7804],
         [-0.7952],
         [-0.8108],
         [-0.8216],
         [-0.8300],
         [-0.8380]],

        [[-0.5042],
         [-0.5442],
         [-0.5786],
         [-0.5705],
         [-0.5791],
         [-0.5887],
         [-0.5994],
         [-0.6106],
         [-0.6250],
         [-0.6405],
         [-0.6627],
         [-0.6943]],

        [[-0.9252],
         [-0.9254],
         [-0.9315],
         [-0.9333],
         [-0.9352],
         [-0.9381],
         [-0.9409],
         [-0.9442],
         [-0.9480],
         [-0.9513],
         [-0.9542],
         [-0.9571]],

        [[-0.9983],
         [-0.9979],
         [-0.9980],
         [-0.9980],
         [-0.9977],
         [-0.9978],
         [-0.9982],
         [-0.9979],
         [-0.9975],
         [-0.9979],
         [-0.9982],
         [-0.9980]],

        [[-0.6721],
         [-0.6719],
         [-0.6816],
         [-0.6910],
         [-0.7077],
         [-0.7195],
         [-0.7335],
         [-0.7473],
         [-0.7668],
         [-0.7834],
         [-0.7927],
         [-0.8028]],

        [[-0.8140],
         [-0.8193],
         [-0.8260],
         [-0.8285],
         [-0.8402],
         [-0.8469],
         [-0.8526],
         [-0.8587],
         [-0.8672],
         [-0.8750],
         [-0.8826],
         [-0.8925]],

        [[-0.8182],
         [-0.8115],
         [-0.8181],
         [-0.8204],
         [-0.8286],
         [-0.8375],
         [-0.8475],
         [-0.8575],
         [-0.8647],
         [-0.8710],
         [-0.8772],
         [-0.8837]],

        [[-0.8601],
         [-0.8553],
         [-0.8529],
         [-0.8584],
         [-0.8652],
         [-0.8710],
         [-0.8763],
         [-0.8821],
         [-0.8892],
         [-0.8956],
         [-0.9019],
         [-0.9090]],

        [[-0.8636],
         [-0.8579],
         [-0.8641],
         [-0.8686],
         [-0.8730],
         [-0.8780],
         [-0.8810],
         [-0.8854],
         [-0.8902],
         [-0.8948],
         [-0.9007],
         [-0.9077]],

        [[-0.6343],
         [-0.6238],
         [-0.6411],
         [-0.6486],
         [-0.6655],
         [-0.6838],
         [-0.6984],
         [-0.7164],
         [-0.7422],
         [-0.7608],
         [-0.7734],
         [-0.7895]],

        [[-0.9650],
         [-0.9678],
         [-0.9694],
         [-0.9708],
         [-0.9727],
         [-0.9742],
         [-0.9748],
         [-0.9755],
         [-0.9768],
         [-0.9780],
         [-0.9792],
         [-0.9799]],

        [[-0.4969],
         [-0.5010],
         [-0.4945],
         [-0.5093],
         [-0.5244],
         [-0.5484],
         [-0.5674],
         [-0.5911],
         [-0.6138],
         [-0.6329],
         [-0.6511],
         [-0.6714]]], device='npu:0')
########


金源东路-乐安北一街未来1h车流预测为:
145.60797;142.3653;124.68371;123.977295;118.46912;112.06354;107.37347;102.9243;97.9799;93.98729;90.43794;86.270546
2023-11-09 13:48:47 [INFO] 127.0.0.1 - - [09/Nov/2023 13:48:47] "POST /TrafficFlowForcast HTTP/1.1" 200 -
!!!!!!!!1
CUDA: True cuda:0
Read configuration file: configurations/PREDICT.conf
graph_signal_matrix_filenamedata/PREDICT/20231101.npy
!!!!num_of_hours 1
!!!!num_of_days 0
!!!!num_of_hours 1
!!!!num_of_weeks 0
!!!!DEVICE cuda:0
!!!!batch_size 12
file: 20231101
load file: data/PREDICT/20231101_r1_d0_w0.npz
ori length: 288 , percent: 1.0 , scale: 288
text_x shape: (288, 15, 3, 12)
after cut text_x shape: (12, 15, 3, 12)
train: torch.Size([288, 15, 1, 12]) torch.Size([288, 15, 12]) torch.Size([288, 15, 12])
val: torch.Size([288, 15, 1, 12]) torch.Size([288, 15, 12]) torch.Size([288, 15, 12])
test: torch.Size([12, 15, 1, 12]) torch.Size([12, 15, 12]) torch.Size([12, 15, 12])
!!!!!!!2
!!!!!!!!3

load weight from: ./experiments/PREDICT/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TE/epoch_100.params


#########
tensor([[[-0.8689]],

        [[-0.7842]],

        [[-0.8954]],

        [[-0.7569]],

        [[-0.5042]],

        [[-0.9252]],

        [[-0.9983]],

        [[-0.6721]],

        [[-0.8140]],

        [[-0.8182]],

        [[-0.8601]],

        [[-0.8636]],

        [[-0.6343]],

        [[-0.9650]],

        [[-0.4969]]], device='npu:0')
########




#########
tensor([[[-0.8689],
         [-0.8615]],

        [[-0.7842],
         [-0.7860]],

        [[-0.8954],
         [-0.8936]],

        [[-0.7569],
         [-0.7481]],

        [[-0.5042],
         [-0.5442]],

        [[-0.9252],
         [-0.9254]],

        [[-0.9983],
         [-0.9979]],

        [[-0.6721],
         [-0.6719]],

        [[-0.8140],
         [-0.8193]],

        [[-0.8182],
         [-0.8115]],

        [[-0.8601],
         [-0.8553]],

        [[-0.8636],
         [-0.8579]],

        [[-0.6343],
         [-0.6238]],

        [[-0.9650],
         [-0.9678]],

        [[-0.4969],
         [-0.5010]]], device='npu:0')
########




#########
tensor([[[-0.8689],
         [-0.8615],
         [-0.8699]],

        [[-0.7842],
         [-0.7860],
         [-0.8016]],

        [[-0.8954],
         [-0.8936],
         [-0.8998]],

        [[-0.7569],
         [-0.7481],
         [-0.7510]],

        [[-0.5042],
         [-0.5442],
         [-0.5786]],

        [[-0.9252],
         [-0.9254],
         [-0.9315]],

        [[-0.9983],
         [-0.9979],
         [-0.9980]],

        [[-0.6721],
         [-0.6719],
         [-0.6816]],

        [[-0.8140],
         [-0.8193],
         [-0.8260]],

        [[-0.8182],
         [-0.8115],
         [-0.8181]],

        [[-0.8601],
         [-0.8553],
         [-0.8529]],

        [[-0.8636],
         [-0.8579],
         [-0.8641]],

        [[-0.6343],
         [-0.6238],
         [-0.6411]],

        [[-0.9650],
         [-0.9678],
         [-0.9694]],

        [[-0.4969],
         [-0.5010],
         [-0.4945]]], device='npu:0')
########




#########
tensor([[[-0.8689],
         [-0.8615],
         [-0.8699],
         [-0.8736]],

        [[-0.7842],
         [-0.7860],
         [-0.8016],
         [-0.8044]],

        [[-0.8954],
         [-0.8936],
         [-0.8998],
         [-0.9022]],

        [[-0.7569],
         [-0.7481],
         [-0.7510],
         [-0.7532]],

        [[-0.5042],
         [-0.5442],
         [-0.5786],
         [-0.5705]],

        [[-0.9252],
         [-0.9254],
         [-0.9315],
         [-0.9333]],

        [[-0.9983],
         [-0.9979],
         [-0.9980],
         [-0.9980]],

        [[-0.6721],
         [-0.6719],
         [-0.6816],
         [-0.6910]],

        [[-0.8140],
         [-0.8193],
         [-0.8260],
         [-0.8285]],

        [[-0.8182],
         [-0.8115],
         [-0.8181],
         [-0.8204]],

        [[-0.8601],
         [-0.8553],
         [-0.8529],
         [-0.8584]],

        [[-0.8636],
         [-0.8579],
         [-0.8641],
         [-0.8686]],

        [[-0.6343],
         [-0.6238],
         [-0.6411],
         [-0.6486]],

        [[-0.9650],
         [-0.9678],
         [-0.9694],
         [-0.9708]],

        [[-0.4969],
         [-0.5010],
         [-0.4945],
         [-0.5093]]], device='npu:0')
########




#########
tensor([[[-0.8689],
         [-0.8615],
         [-0.8699],
         [-0.8736],
         [-0.8788]],

        [[-0.7842],
         [-0.7860],
         [-0.8016],
         [-0.8044],
         [-0.8084]],

        [[-0.8954],
         [-0.8936],
         [-0.8998],
         [-0.9022],
         [-0.9035]],

        [[-0.7569],
         [-0.7481],
         [-0.7510],
         [-0.7532],
         [-0.7577]],

        [[-0.5042],
         [-0.5442],
         [-0.5786],
         [-0.5705],
         [-0.5791]],

        [[-0.9252],
         [-0.9254],
         [-0.9315],
         [-0.9333],
         [-0.9352]],

        [[-0.9983],
         [-0.9979],
         [-0.9980],
         [-0.9980],
         [-0.9977]],

        [[-0.6721],
         [-0.6719],
         [-0.6816],
         [-0.6910],
         [-0.7077]],

        [[-0.8140],
         [-0.8193],
         [-0.8260],
         [-0.8285],
         [-0.8402]],

        [[-0.8182],
         [-0.8115],
         [-0.8181],
         [-0.8204],
         [-0.8286]],

        [[-0.8601],
         [-0.8553],
         [-0.8529],
         [-0.8584],
         [-0.8652]],

        [[-0.8636],
         [-0.8579],
         [-0.8641],
         [-0.8686],
         [-0.8730]],

        [[-0.6343],
         [-0.6238],
         [-0.6411],
         [-0.6486],
         [-0.6655]],

        [[-0.9650],
         [-0.9678],
         [-0.9694],
         [-0.9708],
         [-0.9727]],

        [[-0.4969],
         [-0.5010],
         [-0.4945],
         [-0.5093],
         [-0.5244]]], device='npu:0')
########




#########
tensor([[[-0.8689],
         [-0.8615],
         [-0.8699],
         [-0.8736],
         [-0.8788],
         [-0.8828]],

        [[-0.7842],
         [-0.7860],
         [-0.8016],
         [-0.8044],
         [-0.8084],
         [-0.8136]],

        [[-0.8954],
         [-0.8936],
         [-0.8998],
         [-0.9022],
         [-0.9035],
         [-0.9077]],

        [[-0.7569],
         [-0.7481],
         [-0.7510],
         [-0.7532],
         [-0.7577],
         [-0.7681]],

        [[-0.5042],
         [-0.5442],
         [-0.5786],
         [-0.5705],
         [-0.5791],
         [-0.5887]],

        [[-0.9252],
         [-0.9254],
         [-0.9315],
         [-0.9333],
         [-0.9352],
         [-0.9381]],

        [[-0.9983],
         [-0.9979],
         [-0.9980],
         [-0.9980],
         [-0.9977],
         [-0.9978]],

        [[-0.6721],
         [-0.6719],
         [-0.6816],
         [-0.6910],
         [-0.7077],
         [-0.7195]],

        [[-0.8140],
         [-0.8193],
         [-0.8260],
         [-0.8285],
         [-0.8402],
         [-0.8469]],

        [[-0.8182],
         [-0.8115],
         [-0.8181],
         [-0.8204],
         [-0.8286],
         [-0.8375]],

        [[-0.8601],
         [-0.8553],
         [-0.8529],
         [-0.8584],
         [-0.8652],
         [-0.8710]],

        [[-0.8636],
         [-0.8579],
         [-0.8641],
         [-0.8686],
         [-0.8730],
         [-0.8780]],

        [[-0.6343],
         [-0.6238],
         [-0.6411],
         [-0.6486],
         [-0.6655],
         [-0.6838]],

        [[-0.9650],
         [-0.9678],
         [-0.9694],
         [-0.9708],
         [-0.9727],
         [-0.9742]],

        [[-0.4969],
         [-0.5010],
         [-0.4945],
         [-0.5093],
         [-0.5244],
         [-0.5484]]], device='npu:0')
########




#########
tensor([[[-0.8689],
         [-0.8615],
         [-0.8699],
         [-0.8736],
         [-0.8788],
         [-0.8828],
         [-0.8863]],

        [[-0.7842],
         [-0.7860],
         [-0.8016],
         [-0.8044],
         [-0.8084],
         [-0.8136],
         [-0.8196]],

        [[-0.8954],
         [-0.8936],
         [-0.8998],
         [-0.9022],
         [-0.9035],
         [-0.9077],
         [-0.9115]],

        [[-0.7569],
         [-0.7481],
         [-0.7510],
         [-0.7532],
         [-0.7577],
         [-0.7681],
         [-0.7804]],

        [[-0.5042],
         [-0.5442],
         [-0.5786],
         [-0.5705],
         [-0.5791],
         [-0.5887],
         [-0.5994]],

        [[-0.9252],
         [-0.9254],
         [-0.9315],
         [-0.9333],
         [-0.9352],
         [-0.9381],
         [-0.9409]],

        [[-0.9983],
         [-0.9979],
         [-0.9980],
         [-0.9980],
         [-0.9977],
         [-0.9978],
         [-0.9982]],

        [[-0.6721],
         [-0.6719],
         [-0.6816],
         [-0.6910],
         [-0.7077],
         [-0.7195],
         [-0.7335]],

        [[-0.8140],
         [-0.8193],
         [-0.8260],
         [-0.8285],
         [-0.8402],
         [-0.8469],
         [-0.8526]],

        [[-0.8182],
         [-0.8115],
         [-0.8181],
         [-0.8204],
         [-0.8286],
         [-0.8375],
         [-0.8475]],

        [[-0.8601],
         [-0.8553],
         [-0.8529],
         [-0.8584],
         [-0.8652],
         [-0.8710],
         [-0.8763]],

        [[-0.8636],
         [-0.8579],
         [-0.8641],
         [-0.8686],
         [-0.8730],
         [-0.8780],
         [-0.8810]],

        [[-0.6343],
         [-0.6238],
         [-0.6411],
         [-0.6486],
         [-0.6655],
         [-0.6838],
         [-0.6984]],

        [[-0.9650],
         [-0.9678],
         [-0.9694],
         [-0.9708],
         [-0.9727],
         [-0.9742],
         [-0.9748]],

        [[-0.4969],
         [-0.5010],
         [-0.4945],
         [-0.5093],
         [-0.5244],
         [-0.5484],
         [-0.5674]]], device='npu:0')
########




#########
tensor([[[-0.8689],
         [-0.8615],
         [-0.8699],
         [-0.8736],
         [-0.8788],
         [-0.8828],
         [-0.8863],
         [-0.8904]],

        [[-0.7842],
         [-0.7860],
         [-0.8016],
         [-0.8044],
         [-0.8084],
         [-0.8136],
         [-0.8196],
         [-0.8248]],

        [[-0.8954],
         [-0.8936],
         [-0.8998],
         [-0.9022],
         [-0.9035],
         [-0.9077],
         [-0.9115],
         [-0.9146]],

        [[-0.7569],
         [-0.7481],
         [-0.7510],
         [-0.7532],
         [-0.7577],
         [-0.7681],
         [-0.7804],
         [-0.7952]],

        [[-0.5042],
         [-0.5442],
         [-0.5786],
         [-0.5705],
         [-0.5791],
         [-0.5887],
         [-0.5994],
         [-0.6106]],

        [[-0.9252],
         [-0.9254],
         [-0.9315],
         [-0.9333],
         [-0.9352],
         [-0.9381],
         [-0.9409],
         [-0.9442]],

        [[-0.9983],
         [-0.9979],
         [-0.9980],
         [-0.9980],
         [-0.9977],
         [-0.9978],
         [-0.9982],
         [-0.9979]],

        [[-0.6721],
         [-0.6719],
         [-0.6816],
         [-0.6910],
         [-0.7077],
         [-0.7195],
         [-0.7335],
         [-0.7473]],

        [[-0.8140],
         [-0.8193],
         [-0.8260],
         [-0.8285],
         [-0.8402],
         [-0.8469],
         [-0.8526],
         [-0.8587]],

        [[-0.8182],
         [-0.8115],
         [-0.8181],
         [-0.8204],
         [-0.8286],
         [-0.8375],
         [-0.8475],
         [-0.8575]],

        [[-0.8601],
         [-0.8553],
         [-0.8529],
         [-0.8584],
         [-0.8652],
         [-0.8710],
         [-0.8763],
         [-0.8821]],

        [[-0.8636],
         [-0.8579],
         [-0.8641],
         [-0.8686],
         [-0.8730],
         [-0.8780],
         [-0.8810],
         [-0.8854]],

        [[-0.6343],
         [-0.6238],
         [-0.6411],
         [-0.6486],
         [-0.6655],
         [-0.6838],
         [-0.6984],
         [-0.7164]],

        [[-0.9650],
         [-0.9678],
         [-0.9694],
         [-0.9708],
         [-0.9727],
         [-0.9742],
         [-0.9748],
         [-0.9755]],

        [[-0.4969],
         [-0.5010],
         [-0.4945],
         [-0.5093],
         [-0.5244],
         [-0.5484],
         [-0.5674],
         [-0.5911]]], device='npu:0')
########




#########
tensor([[[-0.8689],
         [-0.8615],
         [-0.8699],
         [-0.8736],
         [-0.8788],
         [-0.8828],
         [-0.8863],
         [-0.8904],
         [-0.8955]],

        [[-0.7842],
         [-0.7860],
         [-0.8016],
         [-0.8044],
         [-0.8084],
         [-0.8136],
         [-0.8196],
         [-0.8248],
         [-0.8304]],

        [[-0.8954],
         [-0.8936],
         [-0.8998],
         [-0.9022],
         [-0.9035],
         [-0.9077],
         [-0.9115],
         [-0.9146],
         [-0.9181]],

        [[-0.7569],
         [-0.7481],
         [-0.7510],
         [-0.7532],
         [-0.7577],
         [-0.7681],
         [-0.7804],
         [-0.7952],
         [-0.8108]],

        [[-0.5042],
         [-0.5442],
         [-0.5786],
         [-0.5705],
         [-0.5791],
         [-0.5887],
         [-0.5994],
         [-0.6106],
         [-0.6250]],

        [[-0.9252],
         [-0.9254],
         [-0.9315],
         [-0.9333],
         [-0.9352],
         [-0.9381],
         [-0.9409],
         [-0.9442],
         [-0.9480]],

        [[-0.9983],
         [-0.9979],
         [-0.9980],
         [-0.9980],
         [-0.9977],
         [-0.9978],
         [-0.9982],
         [-0.9979],
         [-0.9975]],

        [[-0.6721],
         [-0.6719],
         [-0.6816],
         [-0.6910],
         [-0.7077],
         [-0.7195],
         [-0.7335],
         [-0.7473],
         [-0.7668]],

        [[-0.8140],
         [-0.8193],
         [-0.8260],
         [-0.8285],
         [-0.8402],
         [-0.8469],
         [-0.8526],
         [-0.8587],
         [-0.8672]],

        [[-0.8182],
         [-0.8115],
         [-0.8181],
         [-0.8204],
         [-0.8286],
         [-0.8375],
         [-0.8475],
         [-0.8575],
         [-0.8647]],

        [[-0.8601],
         [-0.8553],
         [-0.8529],
         [-0.8584],
         [-0.8652],
         [-0.8710],
         [-0.8763],
         [-0.8821],
         [-0.8892]],

        [[-0.8636],
         [-0.8579],
         [-0.8641],
         [-0.8686],
         [-0.8730],
         [-0.8780],
         [-0.8810],
         [-0.8854],
         [-0.8902]],

        [[-0.6343],
         [-0.6238],
         [-0.6411],
         [-0.6486],
         [-0.6655],
         [-0.6838],
         [-0.6984],
         [-0.7164],
         [-0.7422]],

        [[-0.9650],
         [-0.9678],
         [-0.9694],
         [-0.9708],
         [-0.9727],
         [-0.9742],
         [-0.9748],
         [-0.9755],
         [-0.9768]],

        [[-0.4969],
         [-0.5010],
         [-0.4945],
         [-0.5093],
         [-0.5244],
         [-0.5484],
         [-0.5674],
         [-0.5911],
         [-0.6138]]], device='npu:0')
########




#########
tensor([[[-0.8689],
         [-0.8615],
         [-0.8699],
         [-0.8736],
         [-0.8788],
         [-0.8828],
         [-0.8863],
         [-0.8904],
         [-0.8955],
         [-0.9012]],

        [[-0.7842],
         [-0.7860],
         [-0.8016],
         [-0.8044],
         [-0.8084],
         [-0.8136],
         [-0.8196],
         [-0.8248],
         [-0.8304],
         [-0.8365]],

        [[-0.8954],
         [-0.8936],
         [-0.8998],
         [-0.9022],
         [-0.9035],
         [-0.9077],
         [-0.9115],
         [-0.9146],
         [-0.9181],
         [-0.9219]],

        [[-0.7569],
         [-0.7481],
         [-0.7510],
         [-0.7532],
         [-0.7577],
         [-0.7681],
         [-0.7804],
         [-0.7952],
         [-0.8108],
         [-0.8216]],

        [[-0.5042],
         [-0.5442],
         [-0.5786],
         [-0.5705],
         [-0.5791],
         [-0.5887],
         [-0.5994],
         [-0.6106],
         [-0.6250],
         [-0.6405]],

        [[-0.9252],
         [-0.9254],
         [-0.9315],
         [-0.9333],
         [-0.9352],
         [-0.9381],
         [-0.9409],
         [-0.9442],
         [-0.9480],
         [-0.9513]],

        [[-0.9983],
         [-0.9979],
         [-0.9980],
         [-0.9980],
         [-0.9977],
         [-0.9978],
         [-0.9982],
         [-0.9979],
         [-0.9975],
         [-0.9979]],

        [[-0.6721],
         [-0.6719],
         [-0.6816],
         [-0.6910],
         [-0.7077],
         [-0.7195],
         [-0.7335],
         [-0.7473],
         [-0.7668],
         [-0.7834]],

        [[-0.8140],
         [-0.8193],
         [-0.8260],
         [-0.8285],
         [-0.8402],
         [-0.8469],
         [-0.8526],
         [-0.8587],
         [-0.8672],
         [-0.8750]],

        [[-0.8182],
         [-0.8115],
         [-0.8181],
         [-0.8204],
         [-0.8286],
         [-0.8375],
         [-0.8475],
         [-0.8575],
         [-0.8647],
         [-0.8710]],

        [[-0.8601],
         [-0.8553],
         [-0.8529],
         [-0.8584],
         [-0.8652],
         [-0.8710],
         [-0.8763],
         [-0.8821],
         [-0.8892],
         [-0.8956]],

        [[-0.8636],
         [-0.8579],
         [-0.8641],
         [-0.8686],
         [-0.8730],
         [-0.8780],
         [-0.8810],
         [-0.8854],
         [-0.8902],
         [-0.8948]],

        [[-0.6343],
         [-0.6238],
         [-0.6411],
         [-0.6486],
         [-0.6655],
         [-0.6838],
         [-0.6984],
         [-0.7164],
         [-0.7422],
         [-0.7608]],

        [[-0.9650],
         [-0.9678],
         [-0.9694],
         [-0.9708],
         [-0.9727],
         [-0.9742],
         [-0.9748],
         [-0.9755],
         [-0.9768],
         [-0.9780]],

        [[-0.4969],
         [-0.5010],
         [-0.4945],
         [-0.5093],
         [-0.5244],
         [-0.5484],
         [-0.5674],
         [-0.5911],
         [-0.6138],
         [-0.6329]]], device='npu:0')
########




#########
tensor([[[-0.8689],
         [-0.8615],
         [-0.8699],
         [-0.8736],
         [-0.8788],
         [-0.8828],
         [-0.8863],
         [-0.8904],
         [-0.8955],
         [-0.9012],
         [-0.9085]],

        [[-0.7842],
         [-0.7860],
         [-0.8016],
         [-0.8044],
         [-0.8084],
         [-0.8136],
         [-0.8196],
         [-0.8248],
         [-0.8304],
         [-0.8365],
         [-0.8422]],

        [[-0.8954],
         [-0.8936],
         [-0.8998],
         [-0.9022],
         [-0.9035],
         [-0.9077],
         [-0.9115],
         [-0.9146],
         [-0.9181],
         [-0.9219],
         [-0.9262]],

        [[-0.7569],
         [-0.7481],
         [-0.7510],
         [-0.7532],
         [-0.7577],
         [-0.7681],
         [-0.7804],
         [-0.7952],
         [-0.8108],
         [-0.8216],
         [-0.8300]],

        [[-0.5042],
         [-0.5442],
         [-0.5786],
         [-0.5705],
         [-0.5791],
         [-0.5887],
         [-0.5994],
         [-0.6106],
         [-0.6250],
         [-0.6405],
         [-0.6627]],

        [[-0.9252],
         [-0.9254],
         [-0.9315],
         [-0.9333],
         [-0.9352],
         [-0.9381],
         [-0.9409],
         [-0.9442],
         [-0.9480],
         [-0.9513],
         [-0.9542]],

        [[-0.9983],
         [-0.9979],
         [-0.9980],
         [-0.9980],
         [-0.9977],
         [-0.9978],
         [-0.9982],
         [-0.9979],
         [-0.9975],
         [-0.9979],
         [-0.9982]],

        [[-0.6721],
         [-0.6719],
         [-0.6816],
         [-0.6910],
         [-0.7077],
         [-0.7195],
         [-0.7335],
         [-0.7473],
         [-0.7668],
         [-0.7834],
         [-0.7927]],

        [[-0.8140],
         [-0.8193],
         [-0.8260],
         [-0.8285],
         [-0.8402],
         [-0.8469],
         [-0.8526],
         [-0.8587],
         [-0.8672],
         [-0.8750],
         [-0.8826]],

        [[-0.8182],
         [-0.8115],
         [-0.8181],
         [-0.8204],
         [-0.8286],
         [-0.8375],
         [-0.8475],
         [-0.8575],
         [-0.8647],
         [-0.8710],
         [-0.8772]],

        [[-0.8601],
         [-0.8553],
         [-0.8529],
         [-0.8584],
         [-0.8652],
         [-0.8710],
         [-0.8763],
         [-0.8821],
         [-0.8892],
         [-0.8956],
         [-0.9019]],

        [[-0.8636],
         [-0.8579],
         [-0.8641],
         [-0.8686],
         [-0.8730],
         [-0.8780],
         [-0.8810],
         [-0.8854],
         [-0.8902],
         [-0.8948],
         [-0.9007]],

        [[-0.6343],
         [-0.6238],
         [-0.6411],
         [-0.6486],
         [-0.6655],
         [-0.6838],
         [-0.6984],
         [-0.7164],
         [-0.7422],
         [-0.7608],
         [-0.7734]],

        [[-0.9650],
         [-0.9678],
         [-0.9694],
         [-0.9708],
         [-0.9727],
         [-0.9742],
         [-0.9748],
         [-0.9755],
         [-0.9768],
         [-0.9780],
         [-0.9792]],

        [[-0.4969],
         [-0.5010],
         [-0.4945],
         [-0.5093],
         [-0.5244],
         [-0.5484],
         [-0.5674],
         [-0.5911],
         [-0.6138],
         [-0.6329],
         [-0.6511]]], device='npu:0')
########




#########
tensor([[[-0.8689],
         [-0.8615],
         [-0.8699],
         [-0.8736],
         [-0.8788],
         [-0.8828],
         [-0.8863],
         [-0.8904],
         [-0.8955],
         [-0.9012],
         [-0.9085],
         [-0.9151]],

        [[-0.7842],
         [-0.7860],
         [-0.8016],
         [-0.8044],
         [-0.8084],
         [-0.8136],
         [-0.8196],
         [-0.8248],
         [-0.8304],
         [-0.8365],
         [-0.8422],
         [-0.8486]],

        [[-0.8954],
         [-0.8936],
         [-0.8998],
         [-0.9022],
         [-0.9035],
         [-0.9077],
         [-0.9115],
         [-0.9146],
         [-0.9181],
         [-0.9219],
         [-0.9262],
         [-0.9321]],

        [[-0.7569],
         [-0.7481],
         [-0.7510],
         [-0.7532],
         [-0.7577],
         [-0.7681],
         [-0.7804],
         [-0.7952],
         [-0.8108],
         [-0.8216],
         [-0.8300],
         [-0.8380]],

        [[-0.5042],
         [-0.5442],
         [-0.5786],
         [-0.5705],
         [-0.5791],
         [-0.5887],
         [-0.5994],
         [-0.6106],
         [-0.6250],
         [-0.6405],
         [-0.6627],
         [-0.6943]],

        [[-0.9252],
         [-0.9254],
         [-0.9315],
         [-0.9333],
         [-0.9352],
         [-0.9381],
         [-0.9409],
         [-0.9442],
         [-0.9480],
         [-0.9513],
         [-0.9542],
         [-0.9571]],

        [[-0.9983],
         [-0.9979],
         [-0.9980],
         [-0.9980],
         [-0.9977],
         [-0.9978],
         [-0.9982],
         [-0.9979],
         [-0.9975],
         [-0.9979],
         [-0.9982],
         [-0.9980]],

        [[-0.6721],
         [-0.6719],
         [-0.6816],
         [-0.6910],
         [-0.7077],
         [-0.7195],
         [-0.7335],
         [-0.7473],
         [-0.7668],
         [-0.7834],
         [-0.7927],
         [-0.8028]],

        [[-0.8140],
         [-0.8193],
         [-0.8260],
         [-0.8285],
         [-0.8402],
         [-0.8469],
         [-0.8526],
         [-0.8587],
         [-0.8672],
         [-0.8750],
         [-0.8826],
         [-0.8925]],

        [[-0.8182],
         [-0.8115],
         [-0.8181],
         [-0.8204],
         [-0.8286],
         [-0.8375],
         [-0.8475],
         [-0.8575],
         [-0.8647],
         [-0.8710],
         [-0.8772],
         [-0.8837]],

        [[-0.8601],
         [-0.8553],
         [-0.8529],
         [-0.8584],
         [-0.8652],
         [-0.8710],
         [-0.8763],
         [-0.8821],
         [-0.8892],
         [-0.8956],
         [-0.9019],
         [-0.9090]],

        [[-0.8636],
         [-0.8579],
         [-0.8641],
         [-0.8686],
         [-0.8730],
         [-0.8780],
         [-0.8810],
         [-0.8854],
         [-0.8902],
         [-0.8948],
         [-0.9007],
         [-0.9077]],

        [[-0.6343],
         [-0.6238],
         [-0.6411],
         [-0.6486],
         [-0.6655],
         [-0.6838],
         [-0.6984],
         [-0.7164],
         [-0.7422],
         [-0.7608],
         [-0.7734],
         [-0.7895]],

        [[-0.9650],
         [-0.9678],
         [-0.9694],
         [-0.9708],
         [-0.9727],
         [-0.9742],
         [-0.9748],
         [-0.9755],
         [-0.9768],
         [-0.9780],
         [-0.9792],
         [-0.9799]],

        [[-0.4969],
         [-0.5010],
         [-0.4945],
         [-0.5093],
         [-0.5244],
         [-0.5484],
         [-0.5674],
         [-0.5911],
         [-0.6138],
         [-0.6329],
         [-0.6511],
         [-0.6714]]], device='npu:0')
########


金源东路-双文北街未来1h车流预测为:
36.09037;31.56916;29.552427;28.039032;26.495419;25.040407;24.130194;23.577538;22.822412;21.658718;20.545792;20.004742
2023-11-09 13:49:13 [INFO] 127.0.0.1 - - [09/Nov/2023 13:49:13] "POST /TrafficFlowForcast HTTP/1.1" 200 -
!!!!!!!!1
CUDA: True cuda:0
Read configuration file: configurations/PREDICT.conf
graph_signal_matrix_filenamedata/PREDICT/20231101.npy
!!!!num_of_hours 1
!!!!num_of_days 0
!!!!num_of_hours 1
!!!!num_of_weeks 0
!!!!DEVICE cuda:0
!!!!batch_size 12
file: 20231101
load file: data/PREDICT/20231101_r1_d0_w0.npz
ori length: 288 , percent: 1.0 , scale: 288
text_x shape: (288, 15, 3, 12)
after cut text_x shape: (12, 15, 3, 12)
train: torch.Size([288, 15, 1, 12]) torch.Size([288, 15, 12]) torch.Size([288, 15, 12])
val: torch.Size([288, 15, 1, 12]) torch.Size([288, 15, 12]) torch.Size([288, 15, 12])
test: torch.Size([12, 15, 1, 12]) torch.Size([12, 15, 12]) torch.Size([12, 15, 12])
!!!!!!!2
!!!!!!!!3

load weight from: ./experiments/PREDICT/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TE/epoch_100.params


#########
tensor([[[-0.8689]],

        [[-0.7842]],

        [[-0.8954]],

        [[-0.7569]],

        [[-0.5042]],

        [[-0.9252]],

        [[-0.9983]],

        [[-0.6721]],

        [[-0.8140]],

        [[-0.8182]],

        [[-0.8601]],

        [[-0.8636]],

        [[-0.6343]],

        [[-0.9650]],

        [[-0.4969]]], device='npu:0')
########




#########
tensor([[[-0.8689],
         [-0.8615]],

        [[-0.7842],
         [-0.7860]],

        [[-0.8954],
         [-0.8936]],

        [[-0.7569],
         [-0.7481]],

        [[-0.5042],
         [-0.5442]],

        [[-0.9252],
         [-0.9254]],

        [[-0.9983],
         [-0.9979]],

        [[-0.6721],
         [-0.6719]],

        [[-0.8140],
         [-0.8193]],

        [[-0.8182],
         [-0.8115]],

        [[-0.8601],
         [-0.8553]],

        [[-0.8636],
         [-0.8579]],

        [[-0.6343],
         [-0.6238]],

        [[-0.9650],
         [-0.9678]],

        [[-0.4969],
         [-0.5010]]], device='npu:0')
########




#########
tensor([[[-0.8689],
         [-0.8615],
         [-0.8699]],

        [[-0.7842],
         [-0.7860],
         [-0.8016]],

        [[-0.8954],
         [-0.8936],
         [-0.8998]],

        [[-0.7569],
         [-0.7481],
         [-0.7510]],

        [[-0.5042],
         [-0.5442],
         [-0.5786]],

        [[-0.9252],
         [-0.9254],
         [-0.9315]],

        [[-0.9983],
         [-0.9979],
         [-0.9980]],

        [[-0.6721],
         [-0.6719],
         [-0.6816]],

        [[-0.8140],
         [-0.8193],
         [-0.8260]],

        [[-0.8182],
         [-0.8115],
         [-0.8181]],

        [[-0.8601],
         [-0.8553],
         [-0.8529]],

        [[-0.8636],
         [-0.8579],
         [-0.8641]],

        [[-0.6343],
         [-0.6238],
         [-0.6411]],

        [[-0.9650],
         [-0.9678],
         [-0.9694]],

        [[-0.4969],
         [-0.5010],
         [-0.4945]]], device='npu:0')
########




#########
tensor([[[-0.8689],
         [-0.8615],
         [-0.8699],
         [-0.8736]],

        [[-0.7842],
         [-0.7860],
         [-0.8016],
         [-0.8044]],

        [[-0.8954],
         [-0.8936],
         [-0.8998],
         [-0.9022]],

        [[-0.7569],
         [-0.7481],
         [-0.7510],
         [-0.7532]],

        [[-0.5042],
         [-0.5442],
         [-0.5786],
         [-0.5705]],

        [[-0.9252],
         [-0.9254],
         [-0.9315],
         [-0.9333]],

        [[-0.9983],
         [-0.9979],
         [-0.9980],
         [-0.9980]],

        [[-0.6721],
         [-0.6719],
         [-0.6816],
         [-0.6910]],

        [[-0.8140],
         [-0.8193],
         [-0.8260],
         [-0.8285]],

        [[-0.8182],
         [-0.8115],
         [-0.8181],
         [-0.8204]],

        [[-0.8601],
         [-0.8553],
         [-0.8529],
         [-0.8584]],

        [[-0.8636],
         [-0.8579],
         [-0.8641],
         [-0.8686]],

        [[-0.6343],
         [-0.6238],
         [-0.6411],
         [-0.6486]],

        [[-0.9650],
         [-0.9678],
         [-0.9694],
         [-0.9708]],

        [[-0.4969],
         [-0.5010],
         [-0.4945],
         [-0.5093]]], device='npu:0')
########




#########
tensor([[[-0.8689],
         [-0.8615],
         [-0.8699],
         [-0.8736],
         [-0.8788]],

        [[-0.7842],
         [-0.7860],
         [-0.8016],
         [-0.8044],
         [-0.8084]],

        [[-0.8954],
         [-0.8936],
         [-0.8998],
         [-0.9022],
         [-0.9035]],

        [[-0.7569],
         [-0.7481],
         [-0.7510],
         [-0.7532],
         [-0.7577]],

        [[-0.5042],
         [-0.5442],
         [-0.5786],
         [-0.5705],
         [-0.5791]],

        [[-0.9252],
         [-0.9254],
         [-0.9315],
         [-0.9333],
         [-0.9352]],

        [[-0.9983],
         [-0.9979],
         [-0.9980],
         [-0.9980],
         [-0.9977]],

        [[-0.6721],
         [-0.6719],
         [-0.6816],
         [-0.6910],
         [-0.7077]],

        [[-0.8140],
         [-0.8193],
         [-0.8260],
         [-0.8285],
         [-0.8402]],

        [[-0.8182],
         [-0.8115],
         [-0.8181],
         [-0.8204],
         [-0.8286]],

        [[-0.8601],
         [-0.8553],
         [-0.8529],
         [-0.8584],
         [-0.8652]],

        [[-0.8636],
         [-0.8579],
         [-0.8641],
         [-0.8686],
         [-0.8730]],

        [[-0.6343],
         [-0.6238],
         [-0.6411],
         [-0.6486],
         [-0.6655]],

        [[-0.9650],
         [-0.9678],
         [-0.9694],
         [-0.9708],
         [-0.9727]],

        [[-0.4969],
         [-0.5010],
         [-0.4945],
         [-0.5093],
         [-0.5244]]], device='npu:0')
########




#########
tensor([[[-0.8689],
         [-0.8615],
         [-0.8699],
         [-0.8736],
         [-0.8788],
         [-0.8828]],

        [[-0.7842],
         [-0.7860],
         [-0.8016],
         [-0.8044],
         [-0.8084],
         [-0.8136]],

        [[-0.8954],
         [-0.8936],
         [-0.8998],
         [-0.9022],
         [-0.9035],
         [-0.9077]],

        [[-0.7569],
         [-0.7481],
         [-0.7510],
         [-0.7532],
         [-0.7577],
         [-0.7681]],

        [[-0.5042],
         [-0.5442],
         [-0.5786],
         [-0.5705],
         [-0.5791],
         [-0.5887]],

        [[-0.9252],
         [-0.9254],
         [-0.9315],
         [-0.9333],
         [-0.9352],
         [-0.9381]],

        [[-0.9983],
         [-0.9979],
         [-0.9980],
         [-0.9980],
         [-0.9977],
         [-0.9978]],

        [[-0.6721],
         [-0.6719],
         [-0.6816],
         [-0.6910],
         [-0.7077],
         [-0.7195]],

        [[-0.8140],
         [-0.8193],
         [-0.8260],
         [-0.8285],
         [-0.8402],
         [-0.8469]],

        [[-0.8182],
         [-0.8115],
         [-0.8181],
         [-0.8204],
         [-0.8286],
         [-0.8375]],

        [[-0.8601],
         [-0.8553],
         [-0.8529],
         [-0.8584],
         [-0.8652],
         [-0.8710]],

        [[-0.8636],
         [-0.8579],
         [-0.8641],
         [-0.8686],
         [-0.8730],
         [-0.8780]],

        [[-0.6343],
         [-0.6238],
         [-0.6411],
         [-0.6486],
         [-0.6655],
         [-0.6838]],

        [[-0.9650],
         [-0.9678],
         [-0.9694],
         [-0.9708],
         [-0.9727],
         [-0.9742]],

        [[-0.4969],
         [-0.5010],
         [-0.4945],
         [-0.5093],
         [-0.5244],
         [-0.5484]]], device='npu:0')
########




#########
tensor([[[-0.8689],
         [-0.8615],
         [-0.8699],
         [-0.8736],
         [-0.8788],
         [-0.8828],
         [-0.8863]],

        [[-0.7842],
         [-0.7860],
         [-0.8016],
         [-0.8044],
         [-0.8084],
         [-0.8136],
         [-0.8196]],

        [[-0.8954],
         [-0.8936],
         [-0.8998],
         [-0.9022],
         [-0.9035],
         [-0.9077],
         [-0.9115]],

        [[-0.7569],
         [-0.7481],
         [-0.7510],
         [-0.7532],
         [-0.7577],
         [-0.7681],
         [-0.7804]],

        [[-0.5042],
         [-0.5442],
         [-0.5786],
         [-0.5705],
         [-0.5791],
         [-0.5887],
         [-0.5994]],

        [[-0.9252],
         [-0.9254],
         [-0.9315],
         [-0.9333],
         [-0.9352],
         [-0.9381],
         [-0.9409]],

        [[-0.9983],
         [-0.9979],
         [-0.9980],
         [-0.9980],
         [-0.9977],
         [-0.9978],
         [-0.9982]],

        [[-0.6721],
         [-0.6719],
         [-0.6816],
         [-0.6910],
         [-0.7077],
         [-0.7195],
         [-0.7335]],

        [[-0.8140],
         [-0.8193],
         [-0.8260],
         [-0.8285],
         [-0.8402],
         [-0.8469],
         [-0.8526]],

        [[-0.8182],
         [-0.8115],
         [-0.8181],
         [-0.8204],
         [-0.8286],
         [-0.8375],
         [-0.8475]],

        [[-0.8601],
         [-0.8553],
         [-0.8529],
         [-0.8584],
         [-0.8652],
         [-0.8710],
         [-0.8763]],

        [[-0.8636],
         [-0.8579],
         [-0.8641],
         [-0.8686],
         [-0.8730],
         [-0.8780],
         [-0.8810]],

        [[-0.6343],
         [-0.6238],
         [-0.6411],
         [-0.6486],
         [-0.6655],
         [-0.6838],
         [-0.6984]],

        [[-0.9650],
         [-0.9678],
         [-0.9694],
         [-0.9708],
         [-0.9727],
         [-0.9742],
         [-0.9748]],

        [[-0.4969],
         [-0.5010],
         [-0.4945],
         [-0.5093],
         [-0.5244],
         [-0.5484],
         [-0.5674]]], device='npu:0')
########




#########
tensor([[[-0.8689],
         [-0.8615],
         [-0.8699],
         [-0.8736],
         [-0.8788],
         [-0.8828],
         [-0.8863],
         [-0.8904]],

        [[-0.7842],
         [-0.7860],
         [-0.8016],
         [-0.8044],
         [-0.8084],
         [-0.8136],
         [-0.8196],
         [-0.8248]],

        [[-0.8954],
         [-0.8936],
         [-0.8998],
         [-0.9022],
         [-0.9035],
         [-0.9077],
         [-0.9115],
         [-0.9146]],

        [[-0.7569],
         [-0.7481],
         [-0.7510],
         [-0.7532],
         [-0.7577],
         [-0.7681],
         [-0.7804],
         [-0.7952]],

        [[-0.5042],
         [-0.5442],
         [-0.5786],
         [-0.5705],
         [-0.5791],
         [-0.5887],
         [-0.5994],
         [-0.6106]],

        [[-0.9252],
         [-0.9254],
         [-0.9315],
         [-0.9333],
         [-0.9352],
         [-0.9381],
         [-0.9409],
         [-0.9442]],

        [[-0.9983],
         [-0.9979],
         [-0.9980],
         [-0.9980],
         [-0.9977],
         [-0.9978],
         [-0.9982],
         [-0.9979]],

        [[-0.6721],
         [-0.6719],
         [-0.6816],
         [-0.6910],
         [-0.7077],
         [-0.7195],
         [-0.7335],
         [-0.7473]],

        [[-0.8140],
         [-0.8193],
         [-0.8260],
         [-0.8285],
         [-0.8402],
         [-0.8469],
         [-0.8526],
         [-0.8587]],

        [[-0.8182],
         [-0.8115],
         [-0.8181],
         [-0.8204],
         [-0.8286],
         [-0.8375],
         [-0.8475],
         [-0.8575]],

        [[-0.8601],
         [-0.8553],
         [-0.8529],
         [-0.8584],
         [-0.8652],
         [-0.8710],
         [-0.8763],
         [-0.8821]],

        [[-0.8636],
         [-0.8579],
         [-0.8641],
         [-0.8686],
         [-0.8730],
         [-0.8780],
         [-0.8810],
         [-0.8854]],

        [[-0.6343],
         [-0.6238],
         [-0.6411],
         [-0.6486],
         [-0.6655],
         [-0.6838],
         [-0.6984],
         [-0.7164]],

        [[-0.9650],
         [-0.9678],
         [-0.9694],
         [-0.9708],
         [-0.9727],
         [-0.9742],
         [-0.9748],
         [-0.9755]],

        [[-0.4969],
         [-0.5010],
         [-0.4945],
         [-0.5093],
         [-0.5244],
         [-0.5484],
         [-0.5674],
         [-0.5911]]], device='npu:0')
########




#########
tensor([[[-0.8689],
         [-0.8615],
         [-0.8699],
         [-0.8736],
         [-0.8788],
         [-0.8828],
         [-0.8863],
         [-0.8904],
         [-0.8955]],

        [[-0.7842],
         [-0.7860],
         [-0.8016],
         [-0.8044],
         [-0.8084],
         [-0.8136],
         [-0.8196],
         [-0.8248],
         [-0.8304]],

        [[-0.8954],
         [-0.8936],
         [-0.8998],
         [-0.9022],
         [-0.9035],
         [-0.9077],
         [-0.9115],
         [-0.9146],
         [-0.9181]],

        [[-0.7569],
         [-0.7481],
         [-0.7510],
         [-0.7532],
         [-0.7577],
         [-0.7681],
         [-0.7804],
         [-0.7952],
         [-0.8108]],

        [[-0.5042],
         [-0.5442],
         [-0.5786],
         [-0.5705],
         [-0.5791],
         [-0.5887],
         [-0.5994],
         [-0.6106],
         [-0.6250]],

        [[-0.9252],
         [-0.9254],
         [-0.9315],
         [-0.9333],
         [-0.9352],
         [-0.9381],
         [-0.9409],
         [-0.9442],
         [-0.9480]],

        [[-0.9983],
         [-0.9979],
         [-0.9980],
         [-0.9980],
         [-0.9977],
         [-0.9978],
         [-0.9982],
         [-0.9979],
         [-0.9975]],

        [[-0.6721],
         [-0.6719],
         [-0.6816],
         [-0.6910],
         [-0.7077],
         [-0.7195],
         [-0.7335],
         [-0.7473],
         [-0.7668]],

        [[-0.8140],
         [-0.8193],
         [-0.8260],
         [-0.8285],
         [-0.8402],
         [-0.8469],
         [-0.8526],
         [-0.8587],
         [-0.8672]],

        [[-0.8182],
         [-0.8115],
         [-0.8181],
         [-0.8204],
         [-0.8286],
         [-0.8375],
         [-0.8475],
         [-0.8575],
         [-0.8647]],

        [[-0.8601],
         [-0.8553],
         [-0.8529],
         [-0.8584],
         [-0.8652],
         [-0.8710],
         [-0.8763],
         [-0.8821],
         [-0.8892]],

        [[-0.8636],
         [-0.8579],
         [-0.8641],
         [-0.8686],
         [-0.8730],
         [-0.8780],
         [-0.8810],
         [-0.8854],
         [-0.8902]],

        [[-0.6343],
         [-0.6238],
         [-0.6411],
         [-0.6486],
         [-0.6655],
         [-0.6838],
         [-0.6984],
         [-0.7164],
         [-0.7422]],

        [[-0.9650],
         [-0.9678],
         [-0.9694],
         [-0.9708],
         [-0.9727],
         [-0.9742],
         [-0.9748],
         [-0.9755],
         [-0.9768]],

        [[-0.4969],
         [-0.5010],
         [-0.4945],
         [-0.5093],
         [-0.5244],
         [-0.5484],
         [-0.5674],
         [-0.5911],
         [-0.6138]]], device='npu:0')
########




#########
tensor([[[-0.8689],
         [-0.8615],
         [-0.8699],
         [-0.8736],
         [-0.8788],
         [-0.8828],
         [-0.8863],
         [-0.8904],
         [-0.8955],
         [-0.9012]],

        [[-0.7842],
         [-0.7860],
         [-0.8016],
         [-0.8044],
         [-0.8084],
         [-0.8136],
         [-0.8196],
         [-0.8248],
         [-0.8304],
         [-0.8365]],

        [[-0.8954],
         [-0.8936],
         [-0.8998],
         [-0.9022],
         [-0.9035],
         [-0.9077],
         [-0.9115],
         [-0.9146],
         [-0.9181],
         [-0.9219]],

        [[-0.7569],
         [-0.7481],
         [-0.7510],
         [-0.7532],
         [-0.7577],
         [-0.7681],
         [-0.7804],
         [-0.7952],
         [-0.8108],
         [-0.8216]],

        [[-0.5042],
         [-0.5442],
         [-0.5786],
         [-0.5705],
         [-0.5791],
         [-0.5887],
         [-0.5994],
         [-0.6106],
         [-0.6250],
         [-0.6405]],

        [[-0.9252],
         [-0.9254],
         [-0.9315],
         [-0.9333],
         [-0.9352],
         [-0.9381],
         [-0.9409],
         [-0.9442],
         [-0.9480],
         [-0.9513]],

        [[-0.9983],
         [-0.9979],
         [-0.9980],
         [-0.9980],
         [-0.9977],
         [-0.9978],
         [-0.9982],
         [-0.9979],
         [-0.9975],
         [-0.9979]],

        [[-0.6721],
         [-0.6719],
         [-0.6816],
         [-0.6910],
         [-0.7077],
         [-0.7195],
         [-0.7335],
         [-0.7473],
         [-0.7668],
         [-0.7834]],

        [[-0.8140],
         [-0.8193],
         [-0.8260],
         [-0.8285],
         [-0.8402],
         [-0.8469],
         [-0.8526],
         [-0.8587],
         [-0.8672],
         [-0.8750]],

        [[-0.8182],
         [-0.8115],
         [-0.8181],
         [-0.8204],
         [-0.8286],
         [-0.8375],
         [-0.8475],
         [-0.8575],
         [-0.8647],
         [-0.8710]],

        [[-0.8601],
         [-0.8553],
         [-0.8529],
         [-0.8584],
         [-0.8652],
         [-0.8710],
         [-0.8763],
         [-0.8821],
         [-0.8892],
         [-0.8956]],

        [[-0.8636],
         [-0.8579],
         [-0.8641],
         [-0.8686],
         [-0.8730],
         [-0.8780],
         [-0.8810],
         [-0.8854],
         [-0.8902],
         [-0.8948]],

        [[-0.6343],
         [-0.6238],
         [-0.6411],
         [-0.6486],
         [-0.6655],
         [-0.6838],
         [-0.6984],
         [-0.7164],
         [-0.7422],
         [-0.7608]],

        [[-0.9650],
         [-0.9678],
         [-0.9694],
         [-0.9708],
         [-0.9727],
         [-0.9742],
         [-0.9748],
         [-0.9755],
         [-0.9768],
         [-0.9780]],

        [[-0.4969],
         [-0.5010],
         [-0.4945],
         [-0.5093],
         [-0.5244],
         [-0.5484],
         [-0.5674],
         [-0.5911],
         [-0.6138],
         [-0.6329]]], device='npu:0')
########




#########
tensor([[[-0.8689],
         [-0.8615],
         [-0.8699],
         [-0.8736],
         [-0.8788],
         [-0.8828],
         [-0.8863],
         [-0.8904],
         [-0.8955],
         [-0.9012],
         [-0.9085]],

        [[-0.7842],
         [-0.7860],
         [-0.8016],
         [-0.8044],
         [-0.8084],
         [-0.8136],
         [-0.8196],
         [-0.8248],
         [-0.8304],
         [-0.8365],
         [-0.8422]],

        [[-0.8954],
         [-0.8936],
         [-0.8998],
         [-0.9022],
         [-0.9035],
         [-0.9077],
         [-0.9115],
         [-0.9146],
         [-0.9181],
         [-0.9219],
         [-0.9262]],

        [[-0.7569],
         [-0.7481],
         [-0.7510],
         [-0.7532],
         [-0.7577],
         [-0.7681],
         [-0.7804],
         [-0.7952],
         [-0.8108],
         [-0.8216],
         [-0.8300]],

        [[-0.5042],
         [-0.5442],
         [-0.5786],
         [-0.5705],
         [-0.5791],
         [-0.5887],
         [-0.5994],
         [-0.6106],
         [-0.6250],
         [-0.6405],
         [-0.6627]],

        [[-0.9252],
         [-0.9254],
         [-0.9315],
         [-0.9333],
         [-0.9352],
         [-0.9381],
         [-0.9409],
         [-0.9442],
         [-0.9480],
         [-0.9513],
         [-0.9542]],

        [[-0.9983],
         [-0.9979],
         [-0.9980],
         [-0.9980],
         [-0.9977],
         [-0.9978],
         [-0.9982],
         [-0.9979],
         [-0.9975],
         [-0.9979],
         [-0.9982]],

        [[-0.6721],
         [-0.6719],
         [-0.6816],
         [-0.6910],
         [-0.7077],
         [-0.7195],
         [-0.7335],
         [-0.7473],
         [-0.7668],
         [-0.7834],
         [-0.7927]],

        [[-0.8140],
         [-0.8193],
         [-0.8260],
         [-0.8285],
         [-0.8402],
         [-0.8469],
         [-0.8526],
         [-0.8587],
         [-0.8672],
         [-0.8750],
         [-0.8826]],

        [[-0.8182],
         [-0.8115],
         [-0.8181],
         [-0.8204],
         [-0.8286],
         [-0.8375],
         [-0.8475],
         [-0.8575],
         [-0.8647],
         [-0.8710],
         [-0.8772]],

        [[-0.8601],
         [-0.8553],
         [-0.8529],
         [-0.8584],
         [-0.8652],
         [-0.8710],
         [-0.8763],
         [-0.8821],
         [-0.8892],
         [-0.8956],
         [-0.9019]],

        [[-0.8636],
         [-0.8579],
         [-0.8641],
         [-0.8686],
         [-0.8730],
         [-0.8780],
         [-0.8810],
         [-0.8854],
         [-0.8902],
         [-0.8948],
         [-0.9007]],

        [[-0.6343],
         [-0.6238],
         [-0.6411],
         [-0.6486],
         [-0.6655],
         [-0.6838],
         [-0.6984],
         [-0.7164],
         [-0.7422],
         [-0.7608],
         [-0.7734]],

        [[-0.9650],
         [-0.9678],
         [-0.9694],
         [-0.9708],
         [-0.9727],
         [-0.9742],
         [-0.9748],
         [-0.9755],
         [-0.9768],
         [-0.9780],
         [-0.9792]],

        [[-0.4969],
         [-0.5010],
         [-0.4945],
         [-0.5093],
         [-0.5244],
         [-0.5484],
         [-0.5674],
         [-0.5911],
         [-0.6138],
         [-0.6329],
         [-0.6511]]], device='npu:0')
########




#########
tensor([[[-0.8689],
         [-0.8615],
         [-0.8699],
         [-0.8736],
         [-0.8788],
         [-0.8828],
         [-0.8863],
         [-0.8904],
         [-0.8955],
         [-0.9012],
         [-0.9085],
         [-0.9151]],

        [[-0.7842],
         [-0.7860],
         [-0.8016],
         [-0.8044],
         [-0.8084],
         [-0.8136],
         [-0.8196],
         [-0.8248],
         [-0.8304],
         [-0.8365],
         [-0.8422],
         [-0.8486]],

        [[-0.8954],
         [-0.8936],
         [-0.8998],
         [-0.9022],
         [-0.9035],
         [-0.9077],
         [-0.9115],
         [-0.9146],
         [-0.9181],
         [-0.9219],
         [-0.9262],
         [-0.9321]],

        [[-0.7569],
         [-0.7481],
         [-0.7510],
         [-0.7532],
         [-0.7577],
         [-0.7681],
         [-0.7804],
         [-0.7952],
         [-0.8108],
         [-0.8216],
         [-0.8300],
         [-0.8380]],

        [[-0.5042],
         [-0.5442],
         [-0.5786],
         [-0.5705],
         [-0.5791],
         [-0.5887],
         [-0.5994],
         [-0.6106],
         [-0.6250],
         [-0.6405],
         [-0.6627],
         [-0.6943]],

        [[-0.9252],
         [-0.9254],
         [-0.9315],
         [-0.9333],
         [-0.9352],
         [-0.9381],
         [-0.9409],
         [-0.9442],
         [-0.9480],
         [-0.9513],
         [-0.9542],
         [-0.9571]],

        [[-0.9983],
         [-0.9979],
         [-0.9980],
         [-0.9980],
         [-0.9977],
         [-0.9978],
         [-0.9982],
         [-0.9979],
         [-0.9975],
         [-0.9979],
         [-0.9982],
         [-0.9980]],

        [[-0.6721],
         [-0.6719],
         [-0.6816],
         [-0.6910],
         [-0.7077],
         [-0.7195],
         [-0.7335],
         [-0.7473],
         [-0.7668],
         [-0.7834],
         [-0.7927],
         [-0.8028]],

        [[-0.8140],
         [-0.8193],
         [-0.8260],
         [-0.8285],
         [-0.8402],
         [-0.8469],
         [-0.8526],
         [-0.8587],
         [-0.8672],
         [-0.8750],
         [-0.8826],
         [-0.8925]],

        [[-0.8182],
         [-0.8115],
         [-0.8181],
         [-0.8204],
         [-0.8286],
         [-0.8375],
         [-0.8475],
         [-0.8575],
         [-0.8647],
         [-0.8710],
         [-0.8772],
         [-0.8837]],

        [[-0.8601],
         [-0.8553],
         [-0.8529],
         [-0.8584],
         [-0.8652],
         [-0.8710],
         [-0.8763],
         [-0.8821],
         [-0.8892],
         [-0.8956],
         [-0.9019],
         [-0.9090]],

        [[-0.8636],
         [-0.8579],
         [-0.8641],
         [-0.8686],
         [-0.8730],
         [-0.8780],
         [-0.8810],
         [-0.8854],
         [-0.8902],
         [-0.8948],
         [-0.9007],
         [-0.9077]],

        [[-0.6343],
         [-0.6238],
         [-0.6411],
         [-0.6486],
         [-0.6655],
         [-0.6838],
         [-0.6984],
         [-0.7164],
         [-0.7422],
         [-0.7608],
         [-0.7734],
         [-0.7895]],

        [[-0.9650],
         [-0.9678],
         [-0.9694],
         [-0.9708],
         [-0.9727],
         [-0.9742],
         [-0.9748],
         [-0.9755],
         [-0.9768],
         [-0.9780],
         [-0.9792],
         [-0.9799]],

        [[-0.4969],
         [-0.5010],
         [-0.4945],
         [-0.5093],
         [-0.5244],
         [-0.5484],
         [-0.5674],
         [-0.5911],
         [-0.6138],
         [-0.6329],
         [-0.6511],
         [-0.6714]]], device='npu:0')
########


金湖东路-红莲街未来1h车流预测为:
146.90112;149.90326;138.13914;132.568;127.19471;122.035385;115.94252;110.264145;105.18121;101.30864;98.084694;94.2514
2023-11-09 13:49:51 [INFO] 127.0.0.1 - - [09/Nov/2023 13:49:51] "POST /TrafficFlowForcast HTTP/1.1" 200 -
!!!!!!!!1
CUDA: True cuda:0
Read configuration file: configurations/PREDICT.conf
graph_signal_matrix_filenamedata/PREDICT/20231101.npy
!!!!num_of_hours 1
!!!!num_of_days 0
!!!!num_of_hours 1
!!!!num_of_weeks 0
!!!!DEVICE cuda:0
!!!!batch_size 12
file: 20231101
load file: data/PREDICT/20231101_r1_d0_w0.npz
ori length: 288 , percent: 1.0 , scale: 288
text_x shape: (288, 15, 3, 12)
after cut text_x shape: (12, 15, 3, 12)
train: torch.Size([288, 15, 1, 12]) torch.Size([288, 15, 12]) torch.Size([288, 15, 12])
val: torch.Size([288, 15, 1, 12]) torch.Size([288, 15, 12]) torch.Size([288, 15, 12])
test: torch.Size([12, 15, 1, 12]) torch.Size([12, 15, 12]) torch.Size([12, 15, 12])
!!!!!!!2
!!!!!!!!3

load weight from: ./experiments/PREDICT/MAE_ASTGNN_h1d0w0_layer4_head8_dm64_channel1_dir2_drop0.00_1.00e-03TcontextScaledSAtSE0TE/epoch_100.params


#########
tensor([[[-0.8907]],

        [[-0.8036]],

        [[-0.9086]],

        [[-0.7798]],

        [[-0.5597]],

        [[-0.9389]],

        [[-0.9978]],

        [[-0.7227]],

        [[-0.8432]],

        [[-0.8418]],

        [[-0.8823]],

        [[-0.8846]],

        [[-0.6985]],

        [[-0.9668]],

        [[-0.5713]]], device='npu:0')
########




#########
tensor([[[-0.8907],
         [-0.8788]],

        [[-0.8036],
         [-0.8018]],

        [[-0.9086],
         [-0.9042]],

        [[-0.7798],
         [-0.7763]],

        [[-0.5597],
         [-0.5812]],

        [[-0.9389],
         [-0.9402]],

        [[-0.9978],
         [-0.9973]],

        [[-0.7227],
         [-0.7101]],

        [[-0.8432],
         [-0.8318]],

        [[-0.8418],
         [-0.8287]],

        [[-0.8823],
         [-0.8724]],

        [[-0.8846],
         [-0.8759]],

        [[-0.6985],
         [-0.6899]],

        [[-0.9668],
         [-0.9706]],

        [[-0.5713],
         [-0.5616]]], device='npu:0')
########




#########
tensor([[[-0.8907],
         [-0.8788],
         [-0.8837]],

        [[-0.8036],
         [-0.8018],
         [-0.8185]],

        [[-0.9086],
         [-0.9042],
         [-0.9080]],

        [[-0.7798],
         [-0.7763],
         [-0.7715]],

        [[-0.5597],
         [-0.5812],
         [-0.6039]],

        [[-0.9389],
         [-0.9402],
         [-0.9419]],

        [[-0.9978],
         [-0.9973],
         [-0.9975]],

        [[-0.7227],
         [-0.7101],
         [-0.7173]],

        [[-0.8432],
         [-0.8318],
         [-0.8373]],

        [[-0.8418],
         [-0.8287],
         [-0.8307]],

        [[-0.8823],
         [-0.8724],
         [-0.8744]],

        [[-0.8846],
         [-0.8759],
         [-0.8817]],

        [[-0.6985],
         [-0.6899],
         [-0.6998]],

        [[-0.9668],
         [-0.9706],
         [-0.9721]],

        [[-0.5713],
         [-0.5616],
         [-0.5592]]], device='npu:0')
########




#########
tensor([[[-0.8907],
         [-0.8788],
         [-0.8837],
         [-0.8872]],

        [[-0.8036],
         [-0.8018],
         [-0.8185],
         [-0.8178]],

        [[-0.9086],
         [-0.9042],
         [-0.9080],
         [-0.9087]],

        [[-0.7798],
         [-0.7763],
         [-0.7715],
         [-0.7728]],

        [[-0.5597],
         [-0.5812],
         [-0.6039],
         [-0.5994]],

        [[-0.9389],
         [-0.9402],
         [-0.9419],
         [-0.9436]],

        [[-0.9978],
         [-0.9973],
         [-0.9975],
         [-0.9976]],

        [[-0.7227],
         [-0.7101],
         [-0.7173],
         [-0.7218]],

        [[-0.8432],
         [-0.8318],
         [-0.8373],
         [-0.8442]],

        [[-0.8418],
         [-0.8287],
         [-0.8307],
         [-0.8316]],

        [[-0.8823],
         [-0.8724],
         [-0.8744],
         [-0.8769]],

        [[-0.8846],
         [-0.8759],
         [-0.8817],
         [-0.8804]],

        [[-0.6985],
         [-0.6899],
         [-0.6998],
         [-0.7061]],

        [[-0.9668],
         [-0.9706],
         [-0.9721],
         [-0.9715]],

        [[-0.5713],
         [-0.5616],
         [-0.5592],
         [-0.5546]]], device='npu:0')
########




#########
tensor([[[-0.8907],
         [-0.8788],
         [-0.8837],
         [-0.8872],
         [-0.8908]],

        [[-0.8036],
         [-0.8018],
         [-0.8185],
         [-0.8178],
         [-0.8204]],

        [[-0.9086],
         [-0.9042],
         [-0.9080],
         [-0.9087],
         [-0.9097]],

        [[-0.7798],
         [-0.7763],
         [-0.7715],
         [-0.7728],
         [-0.7776]],

        [[-0.5597],
         [-0.5812],
         [-0.6039],
         [-0.5994],
         [-0.6026]],

        [[-0.9389],
         [-0.9402],
         [-0.9419],
         [-0.9436],
         [-0.9445]],

        [[-0.9978],
         [-0.9973],
         [-0.9975],
         [-0.9976],
         [-0.9973]],

        [[-0.7227],
         [-0.7101],
         [-0.7173],
         [-0.7218],
         [-0.7372]],

        [[-0.8432],
         [-0.8318],
         [-0.8373],
         [-0.8442],
         [-0.8497]],

        [[-0.8418],
         [-0.8287],
         [-0.8307],
         [-0.8316],
         [-0.8350]],

        [[-0.8823],
         [-0.8724],
         [-0.8744],
         [-0.8769],
         [-0.8797]],

        [[-0.8846],
         [-0.8759],
         [-0.8817],
         [-0.8804],
         [-0.8828]],

        [[-0.6985],
         [-0.6899],
         [-0.6998],
         [-0.7061],
         [-0.7253]],

        [[-0.9668],
         [-0.9706],
         [-0.9721],
         [-0.9715],
         [-0.9739]],

        [[-0.5713],
         [-0.5616],
         [-0.5592],
         [-0.5546],
         [-0.5707]]], device='npu:0')
########




#########
tensor([[[-0.8907],
         [-0.8788],
         [-0.8837],
         [-0.8872],
         [-0.8908],
         [-0.8935]],

        [[-0.8036],
         [-0.8018],
         [-0.8185],
         [-0.8178],
         [-0.8204],
         [-0.8265]],

        [[-0.9086],
         [-0.9042],
         [-0.9080],
         [-0.9087],
         [-0.9097],
         [-0.9114]],

        [[-0.7798],
         [-0.7763],
         [-0.7715],
         [-0.7728],
         [-0.7776],
         [-0.7916]],

        [[-0.5597],
         [-0.5812],
         [-0.6039],
         [-0.5994],
         [-0.6026],
         [-0.6141]],

        [[-0.9389],
         [-0.9402],
         [-0.9419],
         [-0.9436],
         [-0.9445],
         [-0.9471]],

        [[-0.9978],
         [-0.9973],
         [-0.9975],
         [-0.9976],
         [-0.9973],
         [-0.9974]],

        [[-0.7227],
         [-0.7101],
         [-0.7173],
         [-0.7218],
         [-0.7372],
         [-0.7491]],

        [[-0.8432],
         [-0.8318],
         [-0.8373],
         [-0.8442],
         [-0.8497],
         [-0.8554]],

        [[-0.8418],
         [-0.8287],
         [-0.8307],
         [-0.8316],
         [-0.8350],
         [-0.8408]],

        [[-0.8823],
         [-0.8724],
         [-0.8744],
         [-0.8769],
         [-0.8797],
         [-0.8859]],

        [[-0.8846],
         [-0.8759],
         [-0.8817],
         [-0.8804],
         [-0.8828],
         [-0.8869]],

        [[-0.6985],
         [-0.6899],
         [-0.6998],
         [-0.7061],
         [-0.7253],
         [-0.7473]],

        [[-0.9668],
         [-0.9706],
         [-0.9721],
         [-0.9715],
         [-0.9739],
         [-0.9755]],

        [[-0.5713],
         [-0.5616],
         [-0.5592],
         [-0.5546],
         [-0.5707],
         [-0.5848]]], device='npu:0')
########




#########
tensor([[[-0.8907],
         [-0.8788],
         [-0.8837],
         [-0.8872],
         [-0.8908],
         [-0.8935],
         [-0.8964]],

        [[-0.8036],
         [-0.8018],
         [-0.8185],
         [-0.8178],
         [-0.8204],
         [-0.8265],
         [-0.8333]],

        [[-0.9086],
         [-0.9042],
         [-0.9080],
         [-0.9087],
         [-0.9097],
         [-0.9114],
         [-0.9145]],

        [[-0.7798],
         [-0.7763],
         [-0.7715],
         [-0.7728],
         [-0.7776],
         [-0.7916],
         [-0.8088]],

        [[-0.5597],
         [-0.5812],
         [-0.6039],
         [-0.5994],
         [-0.6026],
         [-0.6141],
         [-0.6285]],

        [[-0.9389],
         [-0.9402],
         [-0.9419],
         [-0.9436],
         [-0.9445],
         [-0.9471],
         [-0.9493]],

        [[-0.9978],
         [-0.9973],
         [-0.9975],
         [-0.9976],
         [-0.9973],
         [-0.9974],
         [-0.9977]],

        [[-0.7227],
         [-0.7101],
         [-0.7173],
         [-0.7218],
         [-0.7372],
         [-0.7491],
         [-0.7667]],

        [[-0.8432],
         [-0.8318],
         [-0.8373],
         [-0.8442],
         [-0.8497],
         [-0.8554],
         [-0.8643]],

        [[-0.8418],
         [-0.8287],
         [-0.8307],
         [-0.8316],
         [-0.8350],
         [-0.8408],
         [-0.8506]],

        [[-0.8823],
         [-0.8724],
         [-0.8744],
         [-0.8769],
         [-0.8797],
         [-0.8859],
         [-0.8938]],

        [[-0.8846],
         [-0.8759],
         [-0.8817],
         [-0.8804],
         [-0.8828],
         [-0.8869],
         [-0.8889]],

        [[-0.6985],
         [-0.6899],
         [-0.6998],
         [-0.7061],
         [-0.7253],
         [-0.7473],
         [-0.7598]],

        [[-0.9668],
         [-0.9706],
         [-0.9721],
         [-0.9715],
         [-0.9739],
         [-0.9755],
         [-0.9768]],

        [[-0.5713],
         [-0.5616],
         [-0.5592],
         [-0.5546],
         [-0.5707],
         [-0.5848],
         [-0.6008]]], device='npu:0')
########




#########
tensor([[[-0.8907],
         [-0.8788],
         [-0.8837],
         [-0.8872],
         [-0.8908],
         [-0.8935],
         [-0.8964],
         [-0.9042]],

        [[-0.8036],
         [-0.8018],
         [-0.8185],
         [-0.8178],
         [-0.8204],
         [-0.8265],
         [-0.8333],
         [-0.8406]],

        [[-0.9086],
         [-0.9042],
         [-0.9080],
         [-0.9087],
         [-0.9097],
         [-0.9114],
         [-0.9145],
         [-0.9202]],

        [[-0.7798],
         [-0.7763],
         [-0.7715],
         [-0.7728],
         [-0.7776],
         [-0.7916],
         [-0.8088],
         [-0.8229]],

        [[-0.5597],
         [-0.5812],
         [-0.6039],
         [-0.5994],
         [-0.6026],
         [-0.6141],
         [-0.6285],
         [-0.6521]],

        [[-0.9389],
         [-0.9402],
         [-0.9419],
         [-0.9436],
         [-0.9445],
         [-0.9471],
         [-0.9493],
         [-0.9524]],

        [[-0.9978],
         [-0.9973],
         [-0.9975],
         [-0.9976],
         [-0.9973],
         [-0.9974],
         [-0.9977],
         [-0.9974]],

        [[-0.7227],
         [-0.7101],
         [-0.7173],
         [-0.7218],
         [-0.7372],
         [-0.7491],
         [-0.7667],
         [-0.7831]],

        [[-0.8432],
         [-0.8318],
         [-0.8373],
         [-0.8442],
         [-0.8497],
         [-0.8554],
         [-0.8643],
         [-0.8752]],

        [[-0.8418],
         [-0.8287],
         [-0.8307],
         [-0.8316],
         [-0.8350],
         [-0.8408],
         [-0.8506],
         [-0.8630]],

        [[-0.8823],
         [-0.8724],
         [-0.8744],
         [-0.8769],
         [-0.8797],
         [-0.8859],
         [-0.8938],
         [-0.8997]],

        [[-0.8846],
         [-0.8759],
         [-0.8817],
         [-0.8804],
         [-0.8828],
         [-0.8869],
         [-0.8889],
         [-0.8935]],

        [[-0.6985],
         [-0.6899],
         [-0.6998],
         [-0.7061],
         [-0.7253],
         [-0.7473],
         [-0.7598],
         [-0.7726]],

        [[-0.9668],
         [-0.9706],
         [-0.9721],
         [-0.9715],
         [-0.9739],
         [-0.9755],
         [-0.9768],
         [-0.9781]],

        [[-0.5713],
         [-0.5616],
         [-0.5592],
         [-0.5546],
         [-0.5707],
         [-0.5848],
         [-0.6008],
         [-0.6210]]], device='npu:0')
########




#########
tensor([[[-0.8907],
         [-0.8788],
         [-0.8837],
         [-0.8872],
         [-0.8908],
         [-0.8935],
         [-0.8964],
         [-0.9042],
         [-0.9160]],

        [[-0.8036],
         [-0.8018],
         [-0.8185],
         [-0.8178],
         [-0.8204],
         [-0.8265],
         [-0.8333],
         [-0.8406],
         [-0.8495]],

        [[-0.9086],
         [-0.9042],
         [-0.9080],
         [-0.9087],
         [-0.9097],
         [-0.9114],
         [-0.9145],
         [-0.9202],
         [-0.9286]],

        [[-0.7798],
         [-0.7763],
         [-0.7715],
         [-0.7728],
         [-0.7776],
         [-0.7916],
         [-0.8088],
         [-0.8229],
         [-0.8331]],

        [[-0.5597],
         [-0.5812],
         [-0.6039],
         [-0.5994],
         [-0.6026],
         [-0.6141],
         [-0.6285],
         [-0.6521],
         [-0.6859]],

        [[-0.9389],
         [-0.9402],
         [-0.9419],
         [-0.9436],
         [-0.9445],
         [-0.9471],
         [-0.9493],
         [-0.9524],
         [-0.9557]],

        [[-0.9978],
         [-0.9973],
         [-0.9975],
         [-0.9976],
         [-0.9973],
         [-0.9974],
         [-0.9977],
         [-0.9974],
         [-0.9971]],

        [[-0.7227],
         [-0.7101],
         [-0.7173],
         [-0.7218],
         [-0.7372],
         [-0.7491],
         [-0.7667],
         [-0.7831],
         [-0.7936]],

        [[-0.8432],
         [-0.8318],
         [-0.8373],
         [-0.8442],
         [-0.8497],
         [-0.8554],
         [-0.8643],
         [-0.8752],
         [-0.8826]],

        [[-0.8418],
         [-0.8287],
         [-0.8307],
         [-0.8316],
         [-0.8350],
         [-0.8408],
         [-0.8506],
         [-0.8630],
         [-0.8759]],

        [[-0.8823],
         [-0.8724],
         [-0.8744],
         [-0.8769],
         [-0.8797],
         [-0.8859],
         [-0.8938],
         [-0.8997],
         [-0.9065]],

        [[-0.8846],
         [-0.8759],
         [-0.8817],
         [-0.8804],
         [-0.8828],
         [-0.8869],
         [-0.8889],
         [-0.8935],
         [-0.9026]],

        [[-0.6985],
         [-0.6899],
         [-0.6998],
         [-0.7061],
         [-0.7253],
         [-0.7473],
         [-0.7598],
         [-0.7726],
         [-0.7940]],

        [[-0.9668],
         [-0.9706],
         [-0.9721],
         [-0.9715],
         [-0.9739],
         [-0.9755],
         [-0.9768],
         [-0.9781],
         [-0.9794]],

        [[-0.5713],
         [-0.5616],
         [-0.5592],
         [-0.5546],
         [-0.5707],
         [-0.5848],
         [-0.6008],
         [-0.6210],
         [-0.6433]]], device='npu:0')
########




#########
tensor([[[-0.8907],
         [-0.8788],
         [-0.8837],
         [-0.8872],
         [-0.8908],
         [-0.8935],
         [-0.8964],
         [-0.9042],
         [-0.9160],
         [-0.9215]],

        [[-0.8036],
         [-0.8018],
         [-0.8185],
         [-0.8178],
         [-0.8204],
         [-0.8265],
         [-0.8333],
         [-0.8406],
         [-0.8495],
         [-0.8561]],

        [[-0.9086],
         [-0.9042],
         [-0.9080],
         [-0.9087],
         [-0.9097],
         [-0.9114],
         [-0.9145],
         [-0.9202],
         [-0.9286],
         [-0.9338]],

        [[-0.7798],
         [-0.7763],
         [-0.7715],
         [-0.7728],
         [-0.7776],
         [-0.7916],
         [-0.8088],
         [-0.8229],
         [-0.8331],
         [-0.8399]],

        [[-0.5597],
         [-0.5812],
         [-0.6039],
         [-0.5994],
         [-0.6026],
         [-0.6141],
         [-0.6285],
         [-0.6521],
         [-0.6859],
         [-0.7168]],

        [[-0.9389],
         [-0.9402],
         [-0.9419],
         [-0.9436],
         [-0.9445],
         [-0.9471],
         [-0.9493],
         [-0.9524],
         [-0.9557],
         [-0.9576]],

        [[-0.9978],
         [-0.9973],
         [-0.9975],
         [-0.9976],
         [-0.9973],
         [-0.9974],
         [-0.9977],
         [-0.9974],
         [-0.9971],
         [-0.9975]],

        [[-0.7227],
         [-0.7101],
         [-0.7173],
         [-0.7218],
         [-0.7372],
         [-0.7491],
         [-0.7667],
         [-0.7831],
         [-0.7936],
         [-0.8002]],

        [[-0.8432],
         [-0.8318],
         [-0.8373],
         [-0.8442],
         [-0.8497],
         [-0.8554],
         [-0.8643],
         [-0.8752],
         [-0.8826],
         [-0.8886]],

        [[-0.8418],
         [-0.8287],
         [-0.8307],
         [-0.8316],
         [-0.8350],
         [-0.8408],
         [-0.8506],
         [-0.8630],
         [-0.8759],
         [-0.8826]],

        [[-0.8823],
         [-0.8724],
         [-0.8744],
         [-0.8769],
         [-0.8797],
         [-0.8859],
         [-0.8938],
         [-0.8997],
         [-0.9065],
         [-0.9129]],

        [[-0.8846],
         [-0.8759],
         [-0.8817],
         [-0.8804],
         [-0.8828],
         [-0.8869],
         [-0.8889],
         [-0.8935],
         [-0.9026],
         [-0.9084]],

        [[-0.6985],
         [-0.6899],
         [-0.6998],
         [-0.7061],
         [-0.7253],
         [-0.7473],
         [-0.7598],
         [-0.7726],
         [-0.7940],
         [-0.8085]],

        [[-0.9668],
         [-0.9706],
         [-0.9721],
         [-0.9715],
         [-0.9739],
         [-0.9755],
         [-0.9768],
         [-0.9781],
         [-0.9794],
         [-0.9811]],

        [[-0.5713],
         [-0.5616],
         [-0.5592],
         [-0.5546],
         [-0.5707],
         [-0.5848],
         [-0.6008],
         [-0.6210],
         [-0.6433],
         [-0.6673]]], device='npu:0')
########




#########
tensor([[[-0.8907],
         [-0.8788],
         [-0.8837],
         [-0.8872],
         [-0.8908],
         [-0.8935],
         [-0.8964],
         [-0.9042],
         [-0.9160],
         [-0.9215],
         [-0.9231]],

        [[-0.8036],
         [-0.8018],
         [-0.8185],
         [-0.8178],
         [-0.8204],
         [-0.8265],
         [-0.8333],
         [-0.8406],
         [-0.8495],
         [-0.8561],
         [-0.8662]],

        [[-0.9086],
         [-0.9042],
         [-0.9080],
         [-0.9087],
         [-0.9097],
         [-0.9114],
         [-0.9145],
         [-0.9202],
         [-0.9286],
         [-0.9338],
         [-0.9382]],

        [[-0.7798],
         [-0.7763],
         [-0.7715],
         [-0.7728],
         [-0.7776],
         [-0.7916],
         [-0.8088],
         [-0.8229],
         [-0.8331],
         [-0.8399],
         [-0.8471]],

        [[-0.5597],
         [-0.5812],
         [-0.6039],
         [-0.5994],
         [-0.6026],
         [-0.6141],
         [-0.6285],
         [-0.6521],
         [-0.6859],
         [-0.7168],
         [-0.7346]],

        [[-0.9389],
         [-0.9402],
         [-0.9419],
         [-0.9436],
         [-0.9445],
         [-0.9471],
         [-0.9493],
         [-0.9524],
         [-0.9557],
         [-0.9576],
         [-0.9595]],

        [[-0.9978],
         [-0.9973],
         [-0.9975],
         [-0.9976],
         [-0.9973],
         [-0.9974],
         [-0.9977],
         [-0.9974],
         [-0.9971],
         [-0.9975],
         [-0.9978]],

        [[-0.7227],
         [-0.7101],
         [-0.7173],
         [-0.7218],
         [-0.7372],
         [-0.7491],
         [-0.7667],
         [-0.7831],
         [-0.7936],
         [-0.8002],
         [-0.8080]],

        [[-0.8432],
         [-0.8318],
         [-0.8373],
         [-0.8442],
         [-0.8497],
         [-0.8554],
         [-0.8643],
         [-0.8752],
         [-0.8826],
         [-0.8886],
         [-0.8986]],

        [[-0.8418],
         [-0.8287],
         [-0.8307],
         [-0.8316],
         [-0.8350],
         [-0.8408],
         [-0.8506],
         [-0.8630],
         [-0.8759],
         [-0.8826],
         [-0.8853]],

        [[-0.8823],
         [-0.8724],
         [-0.8744],
         [-0.8769],
         [-0.8797],
         [-0.8859],
         [-0.8938],
         [-0.8997],
         [-0.9065],
         [-0.9129],
         [-0.9186]],

        [[-0.8846],
         [-0.8759],
         [-0.8817],
         [-0.8804],
         [-0.8828],
         [-0.8869],
         [-0.8889],
         [-0.8935],
         [-0.9026],
         [-0.9084],
         [-0.9147]],

        [[-0.6985],
         [-0.6899],
         [-0.6998],
         [-0.7061],
         [-0.7253],
         [-0.7473],
         [-0.7598],
         [-0.7726],
         [-0.7940],
         [-0.8085],
         [-0.8189]],

        [[-0.9668],
         [-0.9706],
         [-0.9721],
         [-0.9715],
         [-0.9739],
         [-0.9755],
         [-0.9768],
         [-0.9781],
         [-0.9794],
         [-0.9811],
         [-0.9824]],

        [[-0.5713],
         [-0.5616],
         [-0.5592],
         [-0.5546],
         [-0.5707],
         [-0.5848],
         [-0.6008],
         [-0.6210],
         [-0.6433],
         [-0.6673],
         [-0.6940]]], device='npu:0')
########




#########
tensor([[[-0.8907],
         [-0.8788],
         [-0.8837],
         [-0.8872],
         [-0.8908],
         [-0.8935],
         [-0.8964],
         [-0.9042],
         [-0.9160],
         [-0.9215],
         [-0.9231],
         [-0.9232]],

        [[-0.8036],
         [-0.8018],
         [-0.8185],
         [-0.8178],
         [-0.8204],
         [-0.8265],
         [-0.8333],
         [-0.8406],
         [-0.8495],
         [-0.8561],
         [-0.8662],
         [-0.8723]],

        [[-0.9086],
         [-0.9042],
         [-0.9080],
         [-0.9087],
         [-0.9097],
         [-0.9114],
         [-0.9145],
         [-0.9202],
         [-0.9286],
         [-0.9338],
         [-0.9382],
         [-0.9423]],

        [[-0.7798],
         [-0.7763],
         [-0.7715],
         [-0.7728],
         [-0.7776],
         [-0.7916],
         [-0.8088],
         [-0.8229],
         [-0.8331],
         [-0.8399],
         [-0.8471],
         [-0.8539]],

        [[-0.5597],
         [-0.5812],
         [-0.6039],
         [-0.5994],
         [-0.6026],
         [-0.6141],
         [-0.6285],
         [-0.6521],
         [-0.6859],
         [-0.7168],
         [-0.7346],
         [-0.7468]],

        [[-0.9389],
         [-0.9402],
         [-0.9419],
         [-0.9436],
         [-0.9445],
         [-0.9471],
         [-0.9493],
         [-0.9524],
         [-0.9557],
         [-0.9576],
         [-0.9595],
         [-0.9604]],

        [[-0.9978],
         [-0.9973],
         [-0.9975],
         [-0.9976],
         [-0.9973],
         [-0.9974],
         [-0.9977],
         [-0.9974],
         [-0.9971],
         [-0.9975],
         [-0.9978],
         [-0.9975]],

        [[-0.7227],
         [-0.7101],
         [-0.7173],
         [-0.7218],
         [-0.7372],
         [-0.7491],
         [-0.7667],
         [-0.7831],
         [-0.7936],
         [-0.8002],
         [-0.8080],
         [-0.8170]],

        [[-0.8432],
         [-0.8318],
         [-0.8373],
         [-0.8442],
         [-0.8497],
         [-0.8554],
         [-0.8643],
         [-0.8752],
         [-0.8826],
         [-0.8886],
         [-0.8986],
         [-0.9053]],

        [[-0.8418],
         [-0.8287],
         [-0.8307],
         [-0.8316],
         [-0.8350],
         [-0.8408],
         [-0.8506],
         [-0.8630],
         [-0.8759],
         [-0.8826],
         [-0.8853],
         [-0.8902]],

        [[-0.8823],
         [-0.8724],
         [-0.8744],
         [-0.8769],
         [-0.8797],
         [-0.8859],
         [-0.8938],
         [-0.8997],
         [-0.9065],
         [-0.9129],
         [-0.9186],
         [-0.9245]],

        [[-0.8846],
         [-0.8759],
         [-0.8817],
         [-0.8804],
         [-0.8828],
         [-0.8869],
         [-0.8889],
         [-0.8935],
         [-0.9026],
         [-0.9084],
         [-0.9147],
         [-0.9200]],

        [[-0.6985],
         [-0.6899],
         [-0.6998],
         [-0.7061],
         [-0.7253],
         [-0.7473],
         [-0.7598],
         [-0.7726],
         [-0.7940],
         [-0.8085],
         [-0.8189],
         [-0.8281]],

        [[-0.9668],
         [-0.9706],
         [-0.9721],
         [-0.9715],
         [-0.9739],
         [-0.9755],
         [-0.9768],
         [-0.9781],
         [-0.9794],
         [-0.9811],
         [-0.9824],
         [-0.9832]],

        [[-0.5713],
         [-0.5616],
         [-0.5592],
         [-0.5546],
         [-0.5707],
         [-0.5848],
         [-0.6008],
         [-0.6210],
         [-0.6433],
         [-0.6673],
         [-0.6940],
         [-0.7156]]], device='npu:0')
########


兴贤路-乐民街未来1h车流预测为:
122.57752;108.1981;103.01919;96.509895;92.21521;87.42155;82.78744;79.62224;76.22943;73.142685;70.79378;68.644325
2023-11-09 13:51:37 [INFO] 127.0.0.1 - - [09/Nov/2023 13:51:37] "POST /TrafficFlowForcast HTTP/1.1" 200 -
